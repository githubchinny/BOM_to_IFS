{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOM to IFS file\n",
    "\n",
    "This script reads the latest collated BOM file from the sharepoint directory and extracts the columns we've configured for IFS migration.  This is a stop-gap while we are still working with excel files and will be replaced by a direct migration of data from 3DX straight into IFS - using IFS connect to drop files for processing on an internal IFS queue.\n",
    "\n",
    "\n",
    "Requirements:    \n",
    "A collated bom csv file in Engineering BoM sharepoint directory for the selected project, eg 'T33-BoM-XP_collated_BOM.csv'\n",
    "\n",
    "\n",
    "Inputs:\n",
    "Project name (T50, T50s, T33_XP...)\n",
    "\n",
    "Outputs:   \n",
    "Writes compare files, Delta files, and migration txt files to:   \n",
    "\n",
    "Output dir = sharepoint dir, project, IFS   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import io\n",
    "import openpyxl\n",
    "import excel_formatting\n",
    "import logging\n",
    "import argparse\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_of_script():\n",
    "    '''\n",
    "        determine where this script is running\n",
    "        return either jupyter, ipython, terminal\n",
    "    '''\n",
    "    try:\n",
    "        ipy_str = str(type(get_ipython()))\n",
    "        if 'zmqshell' in ipy_str:\n",
    "            return 'jupyter'\n",
    "        if 'terminal' in ipy_str:\n",
    "            return 'ipython'\n",
    "    except:\n",
    "        return 'terminal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parent_part(BOM):\n",
    "    # reset index before trying to update, otherwise multiple rows get updated\n",
    "    BOM.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    for sgroup, frame in BOM[BOM['Part Level'] > 2].groupby('Sub Group'):\n",
    "        level = {}\n",
    "\n",
    "        previous_parent_part=0\n",
    "\n",
    "        for i, row in frame.iterrows():\n",
    "            current_part_number = row['Part Number']\n",
    "            current_part_level = row['Part Level']\n",
    "            # reset higher levels for each assembly\n",
    "            if current_part_level == 5:\n",
    "                # remove entries from higher levels\n",
    "                keys = [k for k in level if k > 5]\n",
    "                for x in keys:\n",
    "                    del level[x]\n",
    "\n",
    "            # write part number to dictionary under current part level\n",
    "            level[current_part_level] = current_part_number\n",
    "            # update the current_parent_part if we have current part details (info from catia)\n",
    "            # as we've created level 1 and 2 we don't need this check\n",
    "            # print (current_part_level, current_part_number)\n",
    "            level[2] = group_area_dict[sgroup]\n",
    "            if i > 0:\n",
    "            # get the max part level from the level dictionary that's less than current part level\n",
    "                previous_parent_level = max(k for k in level if k < current_part_level)\n",
    "\n",
    "                    # update the parent part\n",
    "                # print (i, \"Parent part {} from previous level {}\".format(level[previous_parent_level], previous_parent_level))\n",
    "                BOM.at[i,'Parent Part'] = level[previous_parent_level]\n",
    "    return BOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_pool_connection():\n",
    "    # connection to database using sqlalchemy\n",
    "    import oracledb\n",
    "    from sqlalchemy import create_engine\n",
    "    from sqlalchemy import text\n",
    "    import db_config\n",
    "    import pandas as pd\n",
    "\n",
    "    # d = r\"C:\\Users\\mark.chinnock\\oracle\\instantclient_21_10\"\n",
    "    # oracledb.init_oracle_client(lib_dir=d)\n",
    "\n",
    "    pool = oracledb.create_pool(user=db_config.user, password=db_config.LIVE_userpwd, dsn=db_config.LIVE_connect_string,\n",
    "                                min=1, max=5, increment=1)\n",
    "\n",
    "    return pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ifs_part_cat(env):\n",
    "    # connection to database using sqlalchemy\n",
    "    import oracledb\n",
    "    from sqlalchemy import create_engine\n",
    "    from sqlalchemy import text\n",
    "    import db_config\n",
    "    import pandas as pd\n",
    "    from contextlib import suppress\n",
    "\n",
    "    # Database Credentials\n",
    "    username = db_config.user\n",
    "    password = db_config.LIVE_userpwd\n",
    "\n",
    "    engine = create_engine(\n",
    "        f'oracle+oracledb://:@',\n",
    "            thick_mode=None,\n",
    "            connect_args={\n",
    "                \"user\": db_config.user,\n",
    "                \"password\": db_config.LIVE_userpwd,\n",
    "                \"host\": db_config.LIVE_host,\n",
    "                \"port\": 1521,\n",
    "                \"service_name\": db_config.LIVE_service\n",
    "        })\n",
    "\n",
    "    query = (\"select distinct c.part_no, c.unit_meas, p.lot_tracking_code, p.serial_tracking_code, p.serial_rule \"\n",
    "            \"from ifsapp.inventory_part c \"\n",
    "            \"left join ifsapp.part_catalog p \"\n",
    "            \"on c.part_no = p.part_no \"\n",
    "            \"where c.part_no like 'T%'\")\n",
    "\n",
    "    try:\n",
    "        with engine.connect() as connection:\n",
    "            # print(connection.scalar(text(\"\"\"SELECT * from IFSAPP.purchase_order_line_all\"\"\")))\n",
    "            query = connection.execute(text(query))\n",
    "\n",
    "        df = pd.DataFrame(query.fetchall())\n",
    "\n",
    "        df.columns = df.columns.str.upper()\n",
    "\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    \n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ignore cols None\n"
     ]
    }
   ],
   "source": [
    "if type_of_script() == 'terminal':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"project\", metavar='Project', type=str, help=\"T50, T50s, T33_XP, etc\") \n",
    "    parser.add_argument(\"timestamps\", metavar='prev_timestamps', type=str, help='timestamp portion from previous migration filename: eg 20230530-2044')\n",
    "    parser.add_argument(\"env\", metavar='Environment', type=str, help='LIVE or Sandbox')\n",
    "    # parser.add_argument(\"incrementer\", metavar='Incrementer', type=int)    \n",
    "    # parser.add_argument('-d', help='Produce Delta files', action='store_true')\n",
    "    parser.add_argument('-i', '--ignore', action='append', help=\"Cols to Ignore from comparison - only needed when we've added/removed a column from migration files\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    project = args.project\n",
    "    prev_timestamps = args.timestamps\n",
    "    env=args.env\n",
    "    ignore_cols_for_comparison = args.ignore\n",
    "    # incrementer=args.incrementer\n",
    "\n",
    "else:\n",
    "    # set defaults if we're running in jupyter\n",
    "    project = 'T50'\n",
    "    # for producing the delta files\n",
    "    prev_timestamps = '20231205-1618'\n",
    "    env = 'LIVE'\n",
    "    ignore_cols_for_comparison = None\n",
    "    # incrementer=6\n",
    "    # env = 'Sandbox'\n",
    "\n",
    "print (\"ignore cols {}\".format(ignore_cols_for_comparison))\n",
    "\n",
    "# personal one drive\n",
    "user_dir = 'C:/Users/USERNAME'\n",
    "# replace USERNAME with current logged on user\n",
    "user_dir = user_dir.replace('USERNAME', os.getlogin())\n",
    "\n",
    "pattern = re.compile(r\"-|_\")\n",
    "project_uc = pattern.split(project.upper())[0]\n",
    "\n",
    "# go find the Engineering BoM directory within the User directory\n",
    "# from glob import glob\n",
    "# sharepoint_dir = glob(user_dir + \"/*gordonmurraydesign*/Documents - Engineering BoM*\", recursive = True)[0]\n",
    "\n",
    "# read in config file\n",
    "config = configparser.ConfigParser()\n",
    "config_file = user_dir + '/user_directory.ini'\n",
    "config.read(config_file)\n",
    "\n",
    "# read in gm_dir and gm_docs from config file\n",
    "try:\n",
    "    gm_dir = config[os.getlogin().lower()]['gm_dir']\n",
    "    gm_docs = config[os.getlogin().lower()]['gmd']\n",
    "except:\n",
    "    print (\"You're probable missing a valid 'C:/Temp/user_directory.ini' file\")\n",
    "    SystemExit(1)\n",
    "\n",
    "# this may find more than one sharepoint directory\n",
    "sharepoint_dir = user_dir + \"/\" + gm_dir + \"/\" + gm_docs\n",
    "\n",
    "base = os.path.join(sharepoint_dir, project)\n",
    "# base = user_dir\n",
    "\n",
    "# where we'll write out the files\n",
    "outdir = os.path.join(base, 'IFS')\n",
    "\n",
    "import time\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M\")\n",
    "\n",
    "# logfile name and location\n",
    "logfile = os.path.join(sharepoint_dir, project, 'IFS', 'logs', \"BOM_to_IFS_{}_{}_log.txt\".format(project, timestr))\n",
    "logging.getLogger('matplotlib').setLevel(logging.WARNING)\n",
    "logging.basicConfig(filename=logfile, filemode='a', level=logging.DEBUG, format='%(asctime)s %(levelname)s:%(message)s')\n",
    "\n",
    "logit = logging.getLogger(__name__)\n",
    "\n",
    "logit.info(\"Starting the process...\")\n",
    "logit.info(\"Creating files for project {}\".format(project))\n",
    "logit.info(\"Running from {}\".format(type_of_script()))\n",
    "logit.info(\"base: {}\".format(base))\n",
    "logit.info(\"outdir: {}\".format(outdir))\n",
    "logit.info(\"Columns being ignore in comparison later (new cols?): {}\".format(ignore_cols_for_comparison))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_area_dict = {\n",
    "'A02-Panels & Closure Systems':project +'-01', \n",
    "'A03-Exterior Systems':project +'-01',\n",
    "'A01-Structure Systems':project +'-01', \n",
    "'B01-Suspension Systems':project +'-02',\n",
    "'C01-Braking Systems':project +'-02', \n",
    "'D01-Steering Systems':project +'-02', \n",
    "'E01-Pedal System':project +'-02',\n",
    "'M01-Control Systems':project +'-03', \n",
    "'M02-Traction Systems':project +'-03',\n",
    "'M03-Electrical Distribution Sys':project +'-03', \n",
    "'M04-Multimedia Systems':project +'-03',\n",
    "'M05-Safety & Security Systems':project +'-03', \n",
    "'M06-Software Systems':project +'-03',\n",
    "'N01-Interior & Trim Systems':project +'-04', \n",
    "'N02-HVAC Systems':project +'-04',\n",
    "'F01-ICE Powertrain Systems':project +'-03', \n",
    "'G01-Transmission Systems':project +'-03',\n",
    "'J01-Pwt NVH & Heatshield Sys':project +'-03', \n",
    "'L01-Cooling Systems':project +'-03',\n",
    "'R01-Styling':project +'-07',\n",
    "'P01-Packaging':project +'-06',\n",
    "'T01-Tooling':project +'-08',\n",
    "'U01-Development':project +'-09',\n",
    "'V01-Accessories':project +'-10'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_type_dict = {\n",
    "'AIH':'Manufactured',\n",
    "'BOF':'Purchased',\n",
    "'BOP':'Purchased',\n",
    "'CON':'Purchased (Raw)',\n",
    "'ENG':'Purchased (Raw)',\n",
    "'FAS':'Purchased (Raw)',\n",
    "'FIP':'Purchased (Raw)',\n",
    "'RAW':'Purchased (Raw)',\n",
    "'MIH':'Manufactured',\n",
    "'POA':'Manufactured',\n",
    "'MOB':'Manufactured'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# existing_bom_file = 'T33-BoM-XP_collated_BOM.csv'\n",
    "existing_bom_file = '{}_collated_BOM.csv'.format(project)\n",
    "\n",
    "path = os.path.join(base, existing_bom_file)\n",
    "try:\n",
    "    with open(path, \"rb\") as f:\n",
    "\n",
    "        existing_bom = pd.read_csv(f, na_values='*', parse_dates=True, low_memory=False) \n",
    "        # sheetnames = [sheet for sheet in f.sheet_names]\n",
    "except (FileNotFoundError):\n",
    "    logit.critical(\"File not found: {}\".format(path))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mark.chinnock\\AppData\\Local\\Temp\\ipykernel_19636\\3085075481.py:13: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  ifs_parts_cat = pd.read_sql(query, connection)\n"
     ]
    }
   ],
   "source": [
    "# call IFS for latest parts catalogue info\n",
    "\n",
    "# get db pool connection\n",
    "pool = db_pool_connection()\n",
    "\n",
    "query = (\"select distinct c.part_no, c.unit_meas, p.lot_tracking_code, p.serial_tracking_code, p.serial_rule \"\n",
    "        \"from ifsapp.inventory_part c \"\n",
    "        \"left join ifsapp.part_catalog p \"\n",
    "        \"on c.part_no = p.part_no \"\n",
    "        \"where c.part_no like 'T%'\")\n",
    "\n",
    "with pool.acquire() as connection:\n",
    "    ifs_parts_cat = pd.read_sql(query, connection)\n",
    "\n",
    "\n",
    "\n",
    "# ifs_parts_cat = get_ifs_part_cat(env)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# was using this to identify data with wrong REV_NO in IFS versus No of rows\n",
    "# query = (\"select * from ifsapp.eng_part_revision_reference where part_no like 'T%'\")\n",
    "\n",
    "# with pool.acquire() as connection:\n",
    "#     test = pd.read_sql(query, connection)\n",
    "\n",
    "# test2 = test.groupby('PART_NO').agg({'PART_NO':'count','REV_NO':'max'}).rename(columns={'PART_NO':'count','REV_NO':'max_rev'})\n",
    "# test3 = test2[test2['count'] < test2['max_rev']].reset_index()\n",
    "# test3[test3['PART_NO'].str.len() < 10]\n",
    "\n",
    "# import xlwings as xw\n",
    "\n",
    "# wb = xw.Book()\n",
    "# ws = wb.sheets[0]\n",
    "\n",
    "# ws['A1'].options(pd.DataFrame, header=1, index=True).value=test3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parts_cat_dict for looking up serial tracking setting\n",
    "parts_cat_dict = {}\t\n",
    "parts_cat_dict = pd.Series(ifs_parts_cat['LOT_TRACKING_CODE'].values,index=ifs_parts_cat['PART_NO']).to_dict()\n",
    "\n",
    "ifs_parts_dict = {}\n",
    "ifs_parts_dict = pd.Series(ifs_parts_cat['UNIT_MEAS'].values,index=ifs_parts_cat['PART_NO']).to_dict()    \n",
    "\n",
    "ifs_serial_tracking = {}\n",
    "ifs_serial_tracking = pd.Series(ifs_parts_cat['SERIAL_TRACKING_CODE'].values,index=ifs_parts_cat['PART_NO']).to_dict()\n",
    "\n",
    "\n",
    "existing_bom.reset_index(inplace=True)\n",
    "existing_bom.rename(columns={'index':'orig_sort'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're not expecting rows for Part level < 3 at the moment.  The assumption is our extracted Excel BoM starts at Part Level 3\n",
    "existing_bom = existing_bom[existing_bom['Part Level'] > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts_reg_file = 'collated_parts_register.xlsx'\n",
    "path = os.path.join(sharepoint_dir, parts_reg_file)\n",
    "try:\n",
    "    parts_reg = pd.read_excel(path)\n",
    "except (FileNotFoundError):\n",
    "    logit.critical(\"File not found: {}\".format(path))\n",
    "\n",
    "engineering_names_file = 'Engineer_names.xlsx'\n",
    "path = os.path.join(sharepoint_dir, engineering_names_file)\n",
    "try:\n",
    "    engineering_names = pd.read_excel(path, sheet_name='Users')\n",
    "except (FileNotFoundError):\n",
    "    logit.critical(\"File not found: {}\".format(path))\n",
    "\n",
    "engineering_names = engineering_names[engineering_names['LAST_NAME'].isna()==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parts_reg_dict for looking up Engineer\n",
    "parts_reg_dict = {}\n",
    "parts_reg_dict = pd.Series(parts_reg['Engineer'].values,index=parts_reg['Part Number']).to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parts_reg_dict for looking up Engineer\n",
    "eng_name_dict = {}\n",
    "eng_name_dict = pd.Series(engineering_names['PERSON_ID'].values,index=engineering_names['LAST_NAME']).to_dict()\n",
    "\n",
    "initials_dict = {}\n",
    "initials_dict = pd.Series(engineering_names['PERSON_ID'].values,index=engineering_names['INITIALS']).to_dict()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think this will either work or fail quietly.\n",
    "existing_bom.rename(columns={'Res. Des. Engineer':'Engineer'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this part is a problem.  It's wrong in the BoM and hasn't been fixed.  It's a U part so won't make it onto a PROD car.\n",
    "i = existing_bom[(existing_bom['Part Number'] == 'T50-U4395*') & (existing_bom['Issue Level'] == 1)].index\n",
    "existing_bom.drop(i, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bring only required rows from BoM\n",
    "\n",
    "This is only relevant for T50 as other BoMs don't have the same 'Requirement' columns.\n",
    "\n",
    "Same logic as MBOM - bring only rows that don't have 4 Ns in the 4 Requirement cols\n",
    "\n",
    "\n",
    "|index|Part Number|\tVP Requirement|\tPP Requirement|\tPS Requirement|\tProd Requirement|\n",
    "|-----|-----------|---------------|---------------|---------------|-----------------|\n",
    "|0|\tT50-A02|\tY|\tY|\tY|\tY|\n",
    "|1|\tT50-A02-01|\tY|\tY|\tY|\tY|\n",
    "|2|\tT50-A0039|\tY|\tY|\tY|\tY|\n",
    "|3|\tT50-A2889|\tY|\tY|\tY|\tY|\n",
    "|4|\tT50-A0041|\tY|\tY|\tY|\tY|\n",
    "...\t...\t...\t...\t...\t...\n",
    "|33927|\tTFF-AA040|\tN|\tN|\tY|\tY|\n",
    "|33928|\tTPP-LZ065|\tN|\tN|\tY|\tY|\n",
    "|33929|\tT50-L0381|\tY|\tY|\tY|\tY|\n",
    "|33930|\tT50-L0373|\tY|\tY|\tY|\tY|\n",
    "|33931|\tTFF-SA888|\tY|\tY|\tY|\tY|\n",
    "\n",
    "\n",
    "\n",
    "convert the 'Y/N' to 1/0 and then sum them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mark.chinnock\\AppData\\Local\\Temp\\ipykernel_19636\\3102930393.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  existing_bom2.drop_duplicates(subset='orig_sort', inplace=True)\n",
      "C:\\Users\\mark.chinnock\\AppData\\Local\\Temp\\ipykernel_19636\\3102930393.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  existing_bom2.sort_values('orig_sort', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "include_aftersales = False\n",
    "\n",
    "if project == 'T50':\n",
    "\n",
    "    # map 'N' to 0 and anything else to 1; then sum them up - keep > 0\n",
    "    existing_bom['VP Requirement'] = np.where(existing_bom['VP Requirement'] == 'N', 0, 1)\n",
    "    existing_bom['PP Requirement'] = np.where(existing_bom['PP Requirement'] == 'N', 0, 1)\n",
    "    existing_bom['PS Requirement'] = np.where(existing_bom['PS Requirement'] == 'N', 0, 1)\n",
    "    existing_bom['Prod Requirement'] = np.where(existing_bom['Prod Requirement'] == 'N', 0, 1)\n",
    "\n",
    "    # make sure we have capitals\n",
    "    existing_bom['Service Identifier'] = existing_bom['Service Identifier'].str.upper()\n",
    "    existing_bom2 = pd.DataFrame()\n",
    "    existing_bom2 = existing_bom[existing_bom.filter(regex='Requirement').sum(axis=1)>0]\n",
    "\n",
    "    # include AFTERSALES build reference rows as well\n",
    "    aftersales = existing_bom[( ( existing_bom['Build References'].str.contains('AFTERSALES', na=False) ) & ( existing_bom['Service Identifier'].isin(['Y','C']) ) ) ]\n",
    "    \n",
    "    # waiting before including these rows\n",
    "    if include_aftersales:\n",
    "        existing_bom2 = pd.concat([existing_bom2, aftersales])\n",
    "        \n",
    "    # make sure we haven't created duplicates\n",
    "    existing_bom2.drop_duplicates(subset='orig_sort', inplace=True)\n",
    "    # make sure we've retained the original order\n",
    "    existing_bom2.sort_values('orig_sort', inplace=True)\n",
    "\n",
    "else:\n",
    "    # pass through existing_bom for all other projects\n",
    "    existing_bom2 = existing_bom\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    existing_bom2.loc[:,'Part Level'] = existing_bom2['Part Level'].replace({r'\\*': ''}, regex=True)\n",
    "    existing_bom2.loc[:,'Part Number'] = existing_bom2['Part Number'].replace({r'\\*': ''}, regex=True)\n",
    "    existing_bom2.loc[:,'Parent Part'] = existing_bom2['Parent Part'].replace({r'\\*': ''}, regex=True)\n",
    "except (ValueError):\n",
    "    logit.error(\"Can't replace asterisk for {}\".format(existing_bom2[['Function Group','Sub Group','Part Number']]))\n",
    "    print (\"Can't replace asterisk for {}\".format(existing_bom2[['Function Group','Sub Group','Part Number']]))\n",
    "\n",
    "existing_bom2.loc[:,'Part Number'] = existing_bom2['Part Number'].str.upper()\n",
    "existing_bom2.loc[:,'Parent Part'] = existing_bom2['Parent Part'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping Engineer data to show what it comes through as\n",
    "cols = ['orig_sort',\n",
    "'Function Group',\n",
    "'Sub Group', \n",
    "'Part Level', \n",
    "'Part Number',\n",
    "'Issue Level',\n",
    "'Part Description', \n",
    "'Part - Qty',\n",
    "'Parent Part', \n",
    "'Source Code',\n",
    "'Weight (KG)',\n",
    "'Release Status',\n",
    "'Engineer',\n",
    "'Service Identifier']\n",
    "\n",
    "# merged = pd.merge(df, existing_bom[cols], on='Part Number', how='left', indicator=True)\n",
    "IFS = existing_bom2[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop Packaging function group as only WIP info.  \n",
    "logit.info(\"Not processing Packaging Function Group\")\n",
    "IFS = IFS[IFS['Function Group'] != 'Packaging']\n",
    "\n",
    "# ignore Accessories for the time being as it is not structured correctly (no level 4)\n",
    "# 2023-05-11 Matt Perry says we need to include these and has corrected the level 4 issue\n",
    "# IFS = IFS[IFS['Function Group'] != 'Accessories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # update Engineer to what we get back from parts_reg.  This is step 1\n",
    "# although this gets set we will ignore it later as the data isn't good enough\n",
    "IFS['Engineer lookup'] = IFS['Part Number'].map(parts_reg_dict).fillna('NO_ENGINEER')\n",
    "\n",
    "# # # next, lookup the surname in the engineer dataframe to get the PERSONID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IFS[IFS['Part Number'] == IFS['Parent Part']]\n",
    "# print (IFS.shape)\n",
    "IFS.dropna(subset='Part Number', inplace=True)\n",
    "# print (IFS.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source_codes_df = pd.DataFrame(data=source_codes_dict, index=source_codes_dict.keys())\n",
    "IFS['Part Type'] = IFS['Source Code'].map(part_type_dict)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empty Structures\n",
    "\n",
    "Lorena says if empty structure for 'BOF,'BOP', then it should be purchased (Raw) rather than purchased\n",
    "\n",
    "Empty structure = following row has part level <= current part level\n",
    "\n",
    "*** Even though this logic works, we don't supply Part Type to the migration process and Migration itself does this empty structure logic instead ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if BOF or BOP and next row's part level <= current part level update to Purchased (Raw)\n",
    "#shift(-1) reads the next row\n",
    "IFS['Part Type'] = np.where(IFS['Source Code'].isin(['BOF','BOP']) & (IFS['Part Level'].shift(-1) <= IFS['Part Level']), 'Purchased (Raw)', IFS['Part Type'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop Source Codes\n",
    "\n",
    "03/03/2023: we are going to drop POA rows completely from the files and then correct the parent part to maintain the structure   \n",
    "08/03/2023: we are dropping ENG rows completely from the dataframe and correct the parent part, if needed   \n",
    "09/03/2023: we are dropping SOP rows as well now   \n",
    "21/07/2023: we want to include SOP rows where there is a Service Identifier of 'Y' or 'C' and it is configured for a PROD vehicle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop source codes\n",
    "# IFS = IFS[~IFS['Source Code'].isin(['POA','ENG'])]\n",
    "IFS = IFS[~IFS['Source Code'].isin(['POA','ENG','SOP'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Source Code</th>\n",
       "      <th>AIH</th>\n",
       "      <th>BOF</th>\n",
       "      <th>BOP</th>\n",
       "      <th>FAS</th>\n",
       "      <th>FIP</th>\n",
       "      <th>FLA</th>\n",
       "      <th>MIH</th>\n",
       "      <th>MOB</th>\n",
       "      <th>SYS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Service Identifier</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>3</td>\n",
       "      <td>178</td>\n",
       "      <td>4</td>\n",
       "      <td>89</td>\n",
       "      <td>315</td>\n",
       "      <td>72</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CF</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N</th>\n",
       "      <td>1244</td>\n",
       "      <td>377</td>\n",
       "      <td>36</td>\n",
       "      <td>306</td>\n",
       "      <td>136</td>\n",
       "      <td>39</td>\n",
       "      <td>103</td>\n",
       "      <td>71</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOT REQ</th>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y</th>\n",
       "      <td>124</td>\n",
       "      <td>1913</td>\n",
       "      <td>169</td>\n",
       "      <td>2211</td>\n",
       "      <td>103</td>\n",
       "      <td>250</td>\n",
       "      <td>23</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Source Code          AIH   BOF  BOP   FAS  FIP  FLA  MIH  MOB  SYS\n",
       "Service Identifier                                                \n",
       "C                      3   178    4    89  315   72    6    8    0\n",
       "CF                     0     0    0     2    0    0    0    0    0\n",
       "N                   1244   377   36   306  136   39  103   71  120\n",
       "NOT REQ                6    16    0    22   29    0    0    0    0\n",
       "Y                    124  1913  169  2211  103  250   23   96    0"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(IFS['Service Identifier'], IFS['Source Code'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Source Code\n",
       "FAS            3914\n",
       "BOF            3798\n",
       "AIH            1998\n",
       "FIP             817\n",
       "FLA             488\n",
       "MIH             292\n",
       "BOP             271\n",
       "MOB             192\n",
       "SYS             121\n",
       "NaN               2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFS[['Source Code']].value_counts(dropna=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level 1 and 2 rows\n",
    "Create a Level 1 and 2 for each function group as they don't exist in the BoM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is clearly wrong but can't work out whether it's worth fixing for IFS.  Needs some proper thought around which rows we add up, and when\n",
    "car_weight = IFS[IFS['Part Level'] >= 5]['Weight (KG)'].sum()\n",
    "\n",
    "level1 = {'Function Group': None,\n",
    " 'Sub Group':None,\n",
    " 'Part Level':1,\n",
    " 'Part Number':[project + '-CAR'],\n",
    " 'Issue Level':1,\n",
    " 'Part Description': ['CAR'],\n",
    " 'Part - Qty':1,\n",
    " 'Parent Part':np.NaN,\n",
    " 'Source Code':'SYS',\n",
    " 'Weight (KG)':car_weight,\n",
    " 'Release Status':'REL',\n",
    " 'Engineer':['Engineer Name']\n",
    " }\n",
    "\n",
    "level2 = {'Function Group': None,\n",
    " 'Sub Group':['BODY SYSTEMS','CHASSIS SYSTEMS','POWERTRAIN SYSTEMS','ELECTRICAL SYSTEMS','INTERIOR & HVAC SYSTEMS','PACKAGING','STYLING','TOOLING','DEVELOPMENT','ACCESSORIES'],\n",
    " 'Part Level':2,\n",
    " 'Part Number':[project + '-01',project + '-02',project + '-03',project + '-04',project + '-05',project + '-06',project + '-07',project + '-08',project + '-09',project + '-10'],\n",
    " 'Issue Level':1,\n",
    " 'Part Description': ['BODY SYSTEMS','CHASSIS SYSTEMS','POWERTRAIN SYSTEMS','ELECTRICAL SYSTEMS','INTERIOR & HVAC SYSTEMS','PACKAGING','STYLING','TOOLING','DEVELOPMENT','ACCESSORIES'],\n",
    " 'Part - Qty':1,\n",
    " 'Parent Part':project + '-CAR',\n",
    " 'Source Code':'SYS',\n",
    " 'Weight (KG)':1000,\n",
    " 'Release Status':'REL',\n",
    " 'Engineer':['Engineer Name','Engineer Name','Engineer Name','Engineer Name','Engineer Name','Engineer Name','Engineer Name','Engineer Name','Engineer Name','Engineer Name']\n",
    " } \n",
    "\n",
    "level1_df = pd.DataFrame(level1)\n",
    "level2_df = pd.DataFrame(level2)\n",
    "\n",
    "level2_df\n",
    "\n",
    "# use the order of concat to put l1 at top, then l2, and then the rest of IFS \n",
    "IFS = pd.concat([level1_df, level2_df, IFS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "IFS.reset_index(inplace=True, drop=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sum function group weights\n",
    "groupby the function groups, update the corresponding level 2 with the sum of that group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_groups_dict = {\n",
    "    project + '-01':['A'], \n",
    "    project + '-02':['B','C','D','E'], \n",
    "    project + '-03':['F','G','J','L'],\n",
    "    project + '-04':['M'],\n",
    "    project + '-05':['N'],\n",
    "    project + '-06':['P'],\n",
    "    # 'T50-07':['R'],\n",
    "    project + '-08':['T'],\n",
    "    # 'T50-09':['U']\n",
    "    }\n",
    "\n",
    "sum_function_groups = IFS.groupby(IFS['Sub Group'].str[:1])['Weight (KG)'].sum()\n",
    "\n",
    "for key in function_groups_dict:\n",
    "    IFS.loc[:,'Weight (KG)'] = np.where(IFS['Part Number'] == key, sum_function_groups[function_groups_dict[key]].sum(), IFS['Weight (KG)'])\n",
    "\n",
    "# key = 'T50-01'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mark.chinnock\\AppData\\Local\\Temp\\ipykernel_19636\\985970340.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  IFS['Weight (KG)'][IFS['Part Number'] == 'T50-CAR'] = car_sum\n"
     ]
    }
   ],
   "source": [
    "# this is a better car weight sum\n",
    "car_sum = 0\n",
    "for key in function_groups_dict:\n",
    "    car_sum = car_sum + IFS['Weight (KG)'][IFS['Part Number'] == key].values\n",
    "    # print (IFS['Weight (KG)'][IFS['Part Number'] == key].values)\n",
    "\n",
    "IFS['Weight (KG)'][IFS['Part Number'] == 'T50-CAR'] = car_sum\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanse Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanse_dataframe(df):\n",
    "\n",
    "    df = df.replace(r'^\\W', '', regex=True)\n",
    "    # remove any number of leading or trailing spaces\n",
    "    df = df.replace(r'^ +| +$', '', regex=True)\n",
    "    # remove strange values at the end of part descriptions\n",
    "    df = df.replace(r'_x000D_', '', regex=True)\n",
    "    # remove any remaining \\n characters\n",
    "    df = df.replace(r'\\n', '', regex=True)\n",
    "\n",
    "    try:\n",
    "        # do individually as might put asterisk in any of the columns\n",
    "        # df = df.replace(regex='\\\\*', value='')\n",
    "        df['Part Level'] = df['Part Level'].replace({r'\\*': ''}, regex=True)\n",
    "        df['Part Number'] = df['Part Number'].replace({r'\\*': ''}, regex=True)\n",
    "        df['Parent Part'] = df['Parent Part'].replace({r'\\*': ''}, regex=True)\n",
    "    except (ValueError):\n",
    "        print (\"Can't replace asterisk for {}\".format(df[['Function Group','Sub Group','Part Number']]))\n",
    "        logit.error(\"Can't replace asterisk for {}\".format(df[['Function Group','Sub Group','Part Number']]))\n",
    "\n",
    "    # leaving these as a reminder that they don't work!  str.replace \\* doesn't actually remove the asterisk! \n",
    "    # dict_df[sheet]['Parent Part'] = dict_df[sheet]['Parent Part'].str.replace(r'\\s*', '', regex=True)\n",
    "    # dict_df[sheet]['Part Number'] = dict_df[sheet]['Part Number'].str.replace(r'\\s*', '', regex=True)\n",
    "    # .replace is better at removing the asterisks\n",
    "    \n",
    "    #uppercase Release Status\n",
    "    df['Release Status'] = df['Release Status'].str.upper()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any whitespace/carriage return characters\n",
    "cleansed_df = cleanse_dataframe(IFS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parent Part\n",
    "Correct the parent part before writing it out.  After removing the POA rows we will need to correct the parent part reference for child parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call update_parent_part which updates the bom directly \n",
    "cleansed_df = update_parent_part(cleansed_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build an SA Index\n",
    "\n",
    "The same parts can be used all over the car/bom.  We need a way of identifying the parts that fall below a specific assembly so that we can collate like parts together and calculate the required quantities.  \n",
    "\n",
    "1. I'm looking for Level 4 or 5s and setting the SA_Index to be the unique row number that came from the 3dx extract.   \n",
    "2. Less than level 4 get their unique row number \n",
    "3. This leaves > level 5 to forward fill with whatever the level 5 row number above was.  This becomes the base for the SA_Index\n",
    "4. Concatenate the individual level and part number (Title) onto the SA_Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleansed_df['SA_Index'] = np.where(cleansed_df['Part Level'].isin([4,5]), cleansed_df['orig_sort'].astype(str), np.nan)\n",
    "cleansed_df['SA_Index'] = np.where(cleansed_df['Part Level'] < 4, cleansed_df['orig_sort'].astype(str), cleansed_df['SA_Index'])\n",
    "# forward fill so that > Level 5 get the same index\n",
    "cleansed_df['SA_Index'] = cleansed_df['SA_Index'].ffill()\n",
    "# don't include Part level in SA_Index\n",
    "# cleansed_df['SA_Index'] = cleansed_df['SA_Index'].astype(str) + '_' + cleansed_df['Part Level'].astype(str) + '_' + cleansed_df['Part Number']\n",
    "cleansed_df['SA_Index'] = cleansed_df['SA_Index'].astype(str) + '_' + cleansed_df['Part Number']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correct multiple issue levels\n",
    "IFS migration cannot handle a part number having more than one revision level in the same migration file.  Need to go through and find the latest Issue Level for each part and update all instances to that\n",
    "\n",
    "Might need to do this later when we have an SA_Index to drop the whole assembly, where applicable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by pn and issue level, unstack to make issue levels columns\n",
    "unstacked = cleansed_df.groupby(['Part Number','Issue Level']).size().unstack()\n",
    "\n",
    "# find number of columns dynamically, as number of unique status controls the number of columns\n",
    "expected_status_count = len(unstacked.columns) - 1\n",
    "\n",
    "unstacked2 = unstacked[unstacked.isna().sum(axis=1)!=expected_status_count].reset_index()\n",
    "dup_parts = unstacked2['Part Number'].tolist()\n",
    "\n",
    "# find level 5 rows (assy) that are in the dup_parts list\n",
    "dup_assy = cleansed_df[(cleansed_df['Part Number'].isin(dup_parts)) & (cleansed_df['Part Level'] ==5)].sort_values(by=['Part Number','Issue Level'])\n",
    "# drop duplicates on pn and keep the last knowing we've sorted them so last row will be max issue level\n",
    "max_assy = dup_assy.drop_duplicates(subset=['Part Number'], keep='last')\n",
    "\n",
    "# substract max_assy from dup_assy, which will leave all the non max rows.\n",
    "SA_to_remove = set(dup_assy['SA_Index'].str.split('_').str[0].drop(max_assy.index).tolist())\n",
    "\n",
    "# create dataframe to write to\n",
    "to_delete = pd.DataFrame()\n",
    "\n",
    "#loop through SA_to_remove and look for the part SA_Index in SA_Index\n",
    "for x in SA_to_remove:\n",
    "    temp_df = cleansed_df[cleansed_df['SA_Index'].str.split('_').str[0] == (x)]\n",
    "    to_delete = pd.concat([to_delete, temp_df])\n",
    "\n",
    "# drop the to_delete datatframe matches on index from cleansed_df\n",
    "cleansed_df.drop(to_delete.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the idea is any parts remaining with different revisions should just use the latest(max) revision\n",
    "# group by the part number and make a dictionary of the max revision\n",
    "\n",
    "max_issue_levels = cleansed_df.groupby(['Part Number'])['Issue Level'].max().reset_index()\n",
    "# max_issue_level for looking up max_issue of a part in the T50 bom.  Shouldn't be a problem in T33\n",
    "max_issue_dict = {}\t\n",
    "max_issue_dict = pd.Series(max_issue_levels['Issue Level'].values,index=max_issue_levels['Part Number']).to_dict()\n",
    "\n",
    "cleansed_df['Issue Level'] = cleansed_df['Part Number'].map(max_issue_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(175, 17)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_delete.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correct FIPs \n",
    "\n",
    "FIP parent part needs to be the previous BOF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleansed_df['BOF_Parent'] = np.where(cleansed_df['Source Code'] == 'BOF', cleansed_df['Part Number'], np.nan)\n",
    "cleansed_df['BOF_Parent'].ffill(inplace=True)\n",
    "cleansed_df['Parent Part'] = np.where(cleansed_df['Source Code'] == 'FIP', cleansed_df['BOF_Parent'], cleansed_df['Parent Part'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group by Duplicate Parts within SA \n",
    "\n",
    "Where we have repeated a part / parent part within an assy, we only need to tell IFS about the structure once and sum up the quantities into one row.\n",
    "\n",
    "Need to make sure the description for a part number is unique throughout the file, regardless of issue level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort out quantity and drop duplicates\n",
    "\n",
    "cleansed_df['Quantity'] = cleansed_df.groupby(['SA_Index','Part Number','Parent Part','Issue Level'])['Part - Qty'].transform('sum')\n",
    "\n",
    "cleansed_df.drop(columns='Part - Qty', inplace=True)\n",
    "cleansed_df.rename(columns={'Quantity':'Part - Qty'}, inplace=True)\n",
    "cleansed_df.drop_duplicates(subset=['SA_Index','Part Number','Parent Part','Issue Level','Part - Qty'], inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is a row with all NaN - remove it\n",
    "cleansed_df.dropna(how='all', inplace=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create structure_df and parts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only create structure_df with 4 cols, using Issue Level once - otherwise we can't rename the columns without it renaming both Issue Levels\n",
    "# we'll add the PART_REV later by mapping to the parent_part_dict\n",
    "\n",
    "structure_cols = [\n",
    "'Parent Part',\n",
    "'Issue Level',\n",
    "'Part Number',\n",
    "'Part - Qty',\n",
    "]\n",
    "\n",
    "# bring in Release Status from BoM as well\n",
    "part_cols = [\n",
    "'Part Number',\n",
    "'Part Level',\n",
    "'Part Description',\n",
    "'Weight (KG)',\n",
    "'Source Code',\n",
    "'Issue Level',\n",
    "'Function Group',\n",
    "'Sub Group',\n",
    "'Engineer lookup',\n",
    "'Release Status'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't need to bring the level1 from cleansed_df into the structure file.  Level1 will always be the first row\n",
    "structure_df = cleansed_df[1:][structure_cols].copy()\n",
    "parts_df = cleansed_df[part_cols].copy()\n",
    "\n",
    "# rename cols to match IFS naming\n",
    "structure_df.rename(columns={\n",
    "    'Parent Part':'PART_NO', \n",
    "    'Part Number':'SUB_PART_NO',\n",
    "    'Part - Qty':'QTY',\n",
    "    'Issue Level':'SUB_PART_REV'   \n",
    "}, inplace=True)\n",
    "\n",
    "parts_df.rename(columns={\n",
    "    'Part Number':'PART_NO',\n",
    "    'Part Description':'DESCRIPTION',\n",
    "    'Weight (KG)':'WEIGHT_NET',\n",
    "    'Function Group':'FUNCTION_GROUP',\n",
    "    'Sub Group':'SUB_GROUP',\n",
    "    'Part Level':'PART_LEVEL',\n",
    "    'Engineer lookup':'PART_RESPONSIBLE',\n",
    "    'Source Code':'SOURCE_CODE',\n",
    "    'Release Status':'RELEASE_STATUS'}\n",
    "    , inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the issue level for the parent part.  Then map this to the PART_REV column of the structure_df\n",
    "\n",
    "parent_part_dict = {}\t\n",
    "# parent_part_dict = pd.Series(IFS['Issue Level'][IFS['Part Number'].isna()==False].values,index=IFS['Part Number'][IFS['Part Number'].isna()==False]).to_dict()\n",
    "# changed this to use cleansed_df\n",
    "parent_part_dict = pd.Series(cleansed_df['Issue Level'][cleansed_df['Part Number'].isna()==False].values,index=cleansed_df['Part Number'][cleansed_df['Part Number'].isna()==False]).to_dict()\n",
    "\n",
    "# this creates the PART_REV column\n",
    "structure_df['PART_REV'] = structure_df['PART_NO'].map(parent_part_dict)\n",
    "\n",
    "# now correct the column ordering for the final file template\n",
    "structure_df = structure_df[['PART_NO', 'PART_REV', 'SUB_PART_NO', 'QTY', 'SUB_PART_REV']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts_df['INFO_TEXT'] = 'Drawing URL?'\n",
    "# map LOT_TRACKING_CODE from existing parts, 22/06/2023 - changed to default to Not Lot Tracking.  This is usual default\n",
    "parts_df['LOT_TRACKING_CODE'] = parts_df['PART_NO'].map(parts_cat_dict).fillna('Not Lot Tracking')\n",
    "# parts_df['LOT_TRACKING_CODE'] = 'Order Based'\n",
    "parts_df['SERIAL_RULE'] = 'Manual'\n",
    "parts_df['CONFIGURABLE'] = 'Not Configured'\n",
    "parts_df['AQUISITION_CODE'] = 'Demand'\n",
    "parts_df['PLANNING_METHOD'] = 'Standard Planned'\n",
    "# use Engineer name from BoM to demonstrate data quality\n",
    "# parts_df['PART_RESPONSIBLE'] = 'Engineer Name'\n",
    "# lookup part issue level for ENG_REV_NO.\n",
    "parts_df['ENG_PART_REV'] = parts_df['Issue Level']\n",
    "# lookup if this is a parent part with a different issue level, otherwise leave as the part issue level\n",
    "parts_df['ENG_PART_REV'] = parts_df['PART_NO'].map(parent_part_dict).fillna(parts_df['ENG_PART_REV'])\n",
    "# look up existing part information for serial tracking first\n",
    "parts_df['SERIAL_TRACKING_CODE'] = parts_df['PART_NO'].map(ifs_serial_tracking)\n",
    "# Use this rule for anything we didn't lookup - Serial Tracking for levels 1, 2 and 5, otherwise keep the current value\n",
    "# parts_df['SERIAL_TRACKING_CODE'] = np.where((parts_df['SERIAL_TRACKING_CODE'].isna() & parts_df['PART_LEVEL'].isin([1,2,5])), 'Serial Tracking', parts_df['SERIAL_TRACKING_CODE'])\n",
    "# and then set all remaining isna() to Not Serial Tracker\n",
    "parts_df['SERIAL_TRACKING_CODE'] = np.where(parts_df['SERIAL_TRACKING_CODE'].isna(), 'Not Serial Tracking', parts_df['SERIAL_TRACKING_CODE'])\n",
    "# map UNIT CODE to existing value, didn't leave any NaN\n",
    "parts_df['UNIT_CODE'] = parts_df['PART_NO'].map(ifs_parts_dict)\n",
    "# AIH, MIH, MOB, POA, SYS source codes are Make, everything else is Buy.  This should match with Manufactured in part_type\n",
    "parts_df['PROVIDE'] = np.where(parts_df['SOURCE_CODE'].isin(['AIH','MIH','MOB','POA','SYS']),'Make','Buy')\n",
    "# map inventory part planning of 'A' and 'P' using the make or buy decision above\n",
    "parts_df['INVENTORY_PART_PLANNING'] = np.where(parts_df['PROVIDE']=='Make', 'P', 'A')\n",
    "# part level 1 is exception to the rule above and will always be PROVIDE = 'Make', INVENTORY_PART_PLANNING = 'A'\n",
    "parts_df['INVENTORY_PART_PLANNING'] = np.where(parts_df['PART_LEVEL'] == 1, 'A', parts_df['INVENTORY_PART_PLANNING'])\n",
    "# create default status for inventory part status of 'A' for purchase/purchase raw, and 'I' for make parts\n",
    "parts_df['INVENTORY_PART_STATUS'] = np.where(parts_df['PROVIDE']=='Make', 'A', 'I')\n",
    "\n",
    "# don't need these columns anymore\n",
    "parts_df.drop(columns=['Issue Level'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default weights to zero where not provided\n",
    "parts_df['WEIGHT_NET'] = np.where(parts_df['WEIGHT_NET'].isna(), 0, parts_df['WEIGHT_NET'])\n",
    "\n",
    "# PART_NO cannot contain lowercase \n",
    "parts_df['PART_NO'] = parts_df['PART_NO'].str.upper()\n",
    "\n",
    "# get rid of negative weights whilst we're waiting for BoM to be corrected\n",
    "parts_df['WEIGHT_NET'] = np.where(parts_df['WEIGHT_NET'] < 0, 0, parts_df['WEIGHT_NET'])\n",
    "\n",
    "# add blank VARIANT and MATURITY cols for future\n",
    "parts_df[['VARIANT','MATURITY']] = np.NaN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENG_REV_NO is an IFS internally incremented number.  We don't have control over it so we are not going to pass it.\n",
    "# migration script in IFS will handle this\n",
    "\n",
    "part_cols_ordered = ['PART_NO',\n",
    "'DESCRIPTION',\n",
    "'WEIGHT_NET',\n",
    "'INFO_TEXT',\n",
    "'UNIT_CODE',\n",
    "'LOT_TRACKING_CODE',\n",
    "'SERIAL_RULE',\n",
    "'SERIAL_TRACKING_CODE',\n",
    "'CONFIGURABLE',\n",
    "'PROVIDE',\n",
    "'AQUISITION_CODE',\n",
    "'PLANNING_METHOD',\n",
    "'PART_RESPONSIBLE',\n",
    "'ENG_PART_REV',\n",
    "# 'ENG_REV_NO',\n",
    "'FUNCTION_GROUP',\n",
    "'SUB_GROUP',\n",
    "'PART_LEVEL',\n",
    "'SOURCE_CODE',\n",
    "'VARIANT',\n",
    "'MATURITY',\n",
    "'INVENTORY_PART_PLANNING',\n",
    "'RELEASE_STATUS',\n",
    "'INVENTORY_PART_STATUS'\n",
    "]\n",
    "\n",
    "parts_df = parts_df[part_cols_ordered]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill blank UNIT_CODE with LTR for FLA\n",
    "parts_df['UNIT_CODE'] = np.where(((parts_df['UNIT_CODE'].isna()) & (parts_df['SOURCE_CODE'] == 'FLA')), 'LTR', parts_df['UNIT_CODE'])\n",
    "\n",
    "# fill remaining blank UNIT_CODE with PCS\n",
    "parts_df['UNIT_CODE'] = parts_df['UNIT_CODE'].fillna('PCS')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map PERSON_ID to Engineer in BoM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's not faff around - just map at a function group level.\n",
    "# none of the other data is good enough / complete enough to work\n",
    "\n",
    "engineer_dict = {\n",
    "    'Body Exterior':'NPETTETT', \n",
    "    'Body Structures':'NBATES', \n",
    "    'Chassis':'JHEWER', \n",
    "    'Electrical':'NHOYLE',\n",
    "    'Body Interior':'NPETTETT',\n",
    "    'Styling':'NHOYLE', \n",
    "    'Tooling':'NHOYLE', \n",
    "    'Development':'NHOYLE', \n",
    "    'Powertrain':'DMORRIS'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts_df['PART_RESPONSIBLE'] = parts_df['FUNCTION_GROUP'].map(engineer_dict)\n",
    "parts_df['PART_RESPONSIBLE'] = np.where(parts_df['PART_RESPONSIBLE'].isna(), 'NHOYLE', parts_df['PART_RESPONSIBLE'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts_df['PART_LEVEL'] = parts_df['PART_LEVEL'].astype(int)\n",
    "# parts_df['ENG_PART_REV'] = np.round(parts_df['ENG_PART_REV'], decimals=2)\n",
    "parts_df['WEIGHT_NET'] = np.round(parts_df['WEIGHT_NET'], decimals = 4)\n",
    "# parts_df['WEIGHT_NET'] = parts_df['WEIGHT_NET'].truncate(8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporary fix for zero and decimal QTY\n",
    "structure_df['QTY'] = np.where(structure_df['QTY'] < 1, 1, structure_df['QTY'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove duplicates from parts_df and structure_df\n",
    "\n",
    "We've done all the work and calculated the quantities.  IFS only need to be told about each of the parts once, and told of the structures once.\n",
    "We don't sum the quantities again - we want 1 steering wheel and reference it in 4 places for the options. Just need to tell IFS once\n",
    "\n",
    "If we have this situation, where T50-B0897 and T50-B0020 part and parent are mentioned more than once, we only need to tell IFS once:\n",
    "\n",
    "```   \n",
    "SA_Index           Part Number  Parent Part  Issue Level\n",
    "17481.0_T50-B0198  T50-B0198    T50-B0020    3.0            1\n",
    "17481.0_T50-B0365  T50-B0365    T50-B0020    1.0            1\n",
    "17481.0_T50-B0841  T50-B0841    T50-B0020    1.0            1\n",
    "17481.0_T50-B0843  T50-B0843    T50-B0020    1.0            1\n",
    "17481.0_T50-B0845  T50-B0845    T50-B0020    1.0            1\n",
    "17481.0_T50-B0847  T50-B0847    T50-B0020    1.0            1\n",
    ">>17481.0_T50-B0897  T50-B0897    T50-B0020    1.0            1<<\n",
    "17481.0_TFF-SA800  TFF-SA800    T50-B0020    1.0            1\n",
    "17501.0_T50-B0198  T50-B0198    T50-B0020    3.0            1\n",
    "17501.0_T50-B0365  T50-B0365    T50-B0020    1.0            1\n",
    "17501.0_T50-B0841  T50-B0841    T50-B0020    1.0            1\n",
    "17501.0_T50-B0843  T50-B0843    T50-B0020    1.0            1\n",
    "17501.0_T50-B0845  T50-B0845    T50-B0020    1.0            1\n",
    "17501.0_T50-B0847  T50-B0847    T50-B0020    1.0            1\n",
    ">>17501.0_T50-B0897  T50-B0897    T50-B0020    1.0            1<<\n",
    "17501.0_TFF-AA059  TFF-AA059    T50-B0020    1.0            1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts_df = parts_df.drop_duplicates(subset=['PART_NO','ENG_PART_REV'])\n",
    "structure_df = structure_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PART_NO</th>\n",
       "      <th>PART_REV</th>\n",
       "      <th>SUB_PART_NO</th>\n",
       "      <th>QTY</th>\n",
       "      <th>SUB_PART_REV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [PART_NO, PART_REV, SUB_PART_NO, QTY, SUB_PART_REV]\n",
       "Index: []"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these structures are still duplicated\n",
    "structure_df[structure_df.duplicated(subset=['PART_NO','SUB_PART_NO'], keep=False)].sort_values(by=['PART_NO','SUB_PART_NO'])\n",
    "# structure_df[(structure_df['PART_NO'] == 'T50-A5285') & (structure_df['SUB_PART_NO'] == 'TFF-SA907')]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Increment ENG_PART_REV\n",
    "\n",
    "If there is a released part already in IFS with the same ENG_PART_REV, and we're changing the structure of the part, IFS will not like it.  We need to increment this value to prevent there being issues in IFS migration.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to stop trailing zeros when writing out to csv\n",
    "parts_df['ENG_PART_REV'] = parts_df['ENG_PART_REV'].apply(str)\n",
    "structure_df['PART_REV'] = structure_df['PART_REV'].apply(str)\n",
    "structure_df['SUB_PART_REV'] = structure_df['SUB_PART_REV'].apply(str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a DELTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "# /**/ makes this recursive through folders in the project specfied\n",
    "from glob import glob\n",
    "\n",
    "try:\n",
    "    glob(outdir + '/**/Structure_*' + prev_timestamps + '.txt', recursive = True)[0]\n",
    "    previous_struct_file = glob(outdir + '/**/Structure_*' + prev_timestamps + '.txt', recursive = True)[0]\n",
    "    previous_part_file = glob(outdir + '/**/Part*' + prev_timestamps + '.txt', recursive = True)[0]\n",
    "except:\n",
    "    print('No previous files found for timestamp {}'.format(prev_timestamps))\n",
    "    print(\"location: {}\".format(outdir + '/**/Part*' + prev_timestamps + '.txt'))\n",
    "    exit()\n",
    "\n",
    "# use dtype in read_csv to capture trailing zeros in ENG_PART_REV\n",
    "path = os.path.join(base, 'IFS', previous_part_file)\n",
    "try:\n",
    "    with open(path, \"rb\") as f:\n",
    "        prev_part = pd.read_csv(f, sep='\\t', dtype={'ENG_PART_REV':str}) \n",
    "        # sheetnames = [sheet for sheet in f.sheet_names]\n",
    "except (FileNotFoundError):\n",
    "    logit.critical(\"File not found: {}\".format(path))\n",
    "\n",
    "try:\n",
    "    path = os.path.join(base, 'IFS', previous_struct_file)\n",
    "    with open(path, \"rb\") as f:\n",
    "        prev_struct = pd.read_csv(f, sep='\\t', dtype={'SUB_PART_REV':str}) \n",
    "        # sheetnames = [sheet for sheet in f.sheet_names]\n",
    "except (FileNotFoundError):\n",
    "    logit.critical(\"File not found: {}\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/Mark.Chinnock/gordonmurraygroup.com/Engineering BoM - Documents\\\\T50\\\\IFS\\\\migrated\\\\Part_T50_LIVE_48_20231205-1618.txt'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previous_part_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derive incrementer from previous part file\n",
    "# incrementer = prev_part['ENG_PART_REV'].str.split('.').str[-1].astype(int).unique()\n",
    "incrementer = prev_part['ENG_PART_REV'].astype(str).str.split('.').str[-1].astype(int).unique()\n",
    "try:\n",
    "    len(incrementer) == 1\n",
    "except:\n",
    "    logit.exception(\"More than one migration revision found in previous file\")\n",
    "    raise\n",
    "\n",
    "incrementer = incrementer[0]\n",
    "incrementer += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is a limit of 6 chars on length of PART_REV fields.  We need to drop the minor revisioning for T50 as it's not being used anyway\n",
    "if project == 'T50':\n",
    "    parts_df['ENG_PART_REV'] = parts_df['ENG_PART_REV'].str.split('.').str[0]\n",
    "    structure_df['PART_REV'] = structure_df['PART_REV'].str.split('.').str[0]\n",
    "    structure_df['SUB_PART_REV'] = structure_df['SUB_PART_REV'].str.split('.').str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the amount we'll add to the ENG_PART_REV to avoid issues in IFS\n",
    "# make a string version of the incrementor\n",
    "str_incr = '.' + str(incrementer)\n",
    "parts_df['ENG_PART_REV'] = parts_df['ENG_PART_REV'] + str_incr\n",
    "structure_df['PART_REV'] = structure_df['PART_REV'] + str_incr\n",
    "structure_df['SUB_PART_REV'] = structure_df['SUB_PART_REV'] + str_incr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no cols to ignore\n"
     ]
    }
   ],
   "source": [
    "# build a dictionary from a copy\n",
    "dict_part_compare = {1:prev_part.copy(),2:parts_df.copy()}\n",
    "dict_struct_compare = {1:prev_struct.copy(),2:structure_df.copy()}\n",
    "\n",
    "if project == 'T50':\n",
    "    # before comparision we need to get rid of the end migration revision as we know it will be different.\n",
    "    # We can do this by converting to int and dropping everything after the decimal place\n",
    "    # dict_part_compare[1]['ENG_PART_REV'] = dict_part_compare[1]['ENG_PART_REV'].str[0]\n",
    "    # dict_part_compare[2]['ENG_PART_REV'] = dict_part_compare[2]['ENG_PART_REV'].str[0]\n",
    "    # dict_struct_compare[1]['PART_REV'] = dict_struct_compare[1]['PART_REV'].str[0]\n",
    "    # dict_struct_compare[2]['PART_REV'] = dict_struct_compare[2]['PART_REV'].str[0]\n",
    "    # dict_struct_compare[1]['SUB_PART_REV'] = dict_struct_compare[1]['SUB_PART_REV'].str[0]\n",
    "    # dict_struct_compare[2]['SUB_PART_REV'] = dict_struct_compare[2]['SUB_PART_REV'].str[0]\n",
    "    dict_part_compare[1]['ENG_PART_REV'] = dict_part_compare[1]['ENG_PART_REV'].astype(str).str.split('.').str[0]\n",
    "    dict_part_compare[2]['ENG_PART_REV'] = dict_part_compare[2]['ENG_PART_REV'].astype(str).str.split('.').str[0]\n",
    "    dict_struct_compare[1]['PART_REV'] = dict_struct_compare[1]['PART_REV'].astype(str).str.split('.').str[0]\n",
    "    dict_struct_compare[2]['PART_REV'] = dict_struct_compare[2]['PART_REV'].astype(str).str.split('.').str[0]\n",
    "    dict_struct_compare[1]['SUB_PART_REV'] = dict_struct_compare[1]['SUB_PART_REV'].astype(str).str.split('.').str[0]\n",
    "    dict_struct_compare[2]['SUB_PART_REV'] = dict_struct_compare[2]['SUB_PART_REV'].astype(str).str.split('.').str[0]    \n",
    "\n",
    "else:\n",
    "    # before comparision we need to get rid of the end migration revision as we know it will be different\n",
    "    dict_part_compare[1]['ENG_PART_REV'] = dict_part_compare[1]['ENG_PART_REV'].str[:3]\n",
    "    dict_part_compare[2]['ENG_PART_REV'] = dict_part_compare[2]['ENG_PART_REV'].str[:3]\n",
    "    dict_struct_compare[1]['PART_REV'] = dict_struct_compare[1]['PART_REV'].str[:3]\n",
    "    dict_struct_compare[2]['PART_REV'] = dict_struct_compare[2]['PART_REV'].str[:3]\n",
    "    dict_struct_compare[1]['SUB_PART_REV'] = dict_struct_compare[1]['SUB_PART_REV'].str[:3]\n",
    "    dict_struct_compare[2]['SUB_PART_REV'] = dict_struct_compare[2]['SUB_PART_REV'].str[:3]\n",
    "\n",
    "dict_struct_compare2=pd.concat(dict_struct_compare)\n",
    "dict_part_compare2=pd.concat(dict_part_compare)\n",
    "\n",
    "# ignore_cols = ['WEIGHT_NET']\n",
    "dict_part_compare2.WEIGHT_NET = np.round(dict_part_compare2.WEIGHT_NET,4).astype(str)\n",
    "\n",
    "subset_cols = []\n",
    "if ignore_cols_for_comparison is not None:\n",
    "    print (\"cols to ignore {}\".format(ignore_cols_for_comparison))\n",
    "    subset_cols = dict_part_compare2.drop(columns=ignore_cols_for_comparison).columns\n",
    "    # delta_parts and delta_struct have the rows with changes\n",
    "    delta_parts = dict_part_compare2[dict_part_compare2['PART_LEVEL']>=5].drop_duplicates(subset=subset_cols, keep=False)\n",
    "\n",
    "else:\n",
    "    print (\"no cols to ignore\")\n",
    "    delta_parts = dict_part_compare2[dict_part_compare2['PART_LEVEL']>=5].drop_duplicates(keep=False)\n",
    "\n",
    "delta_struct = dict_struct_compare2.drop_duplicates(keep=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the PART_NO of any individual PARTs that have changed and the SUB_PART_NO of any parent/assembly that's changed\n",
    "# changed_parts = set(delta_parts['PART_NO'].tolist() + delta_struct['PART_NO'].tolist())\n",
    "changed_parts = set(delta_parts['PART_NO'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the assembly (SA_Index) for any assembly that has those Part numbers\n",
    "delta_sa_index = cleansed_df['SA_Index'].str.split('_').str[0][cleansed_df['Part Number'].isin(changed_parts)].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to give us a unique list\n",
    "sa_set = set(delta_sa_index)\n",
    "\n",
    "delta_df = pd.DataFrame()\n",
    "rel_delta_df = pd.DataFrame()\n",
    "unrel_delta_df = pd.DataFrame()\n",
    "\n",
    "# create regex pattern for word match at start of string followed by any number of chars\n",
    "if len(sa_set) > 0:\n",
    "    pat = '|'.join(r\"\\b^{}.*\\b\".format(x) for x in sa_set)\n",
    "    # build the delta_df\n",
    "    delta_df = cleansed_df[cleansed_df['SA_Index'].str.contains(pat)]\n",
    "    # sort it\n",
    "    delta_df = delta_df.sort_values('orig_sort')\n",
    "else:\n",
    "    logit.warning(\"No changes found for this delta\")\n",
    "    print (\"No changes found for this delta\")\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for unreleased parts/assemblies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# find unreleased parts/assemblies.  We can't pass any assembly to IFS that isn't completely released.\n",
    "# must check there is something in the sa_set\n",
    "if len(sa_set) > 0:\n",
    "    unrel_sa_set = set(delta_df['SA_Index'][delta_df['Release Status'] != 'REL'].str.split('_').str[0])\n",
    "\n",
    "# get the fully released assemblies by ignoring the ones containing unreleased sa_index\n",
    "# must check there is something in the unrel_sa_set\n",
    "if len(unrel_sa_set) > 0:\n",
    "    pat = '|'.join(r\"\\b^{}.*\\b\".format(x) for x in unrel_sa_set)\n",
    "    rel_delta_df = delta_df[~delta_df['SA_Index'].str.contains(pat)]\n",
    "    # get the unrel rows for writing out the warning messages\n",
    "    unrel_delta_df = delta_df[delta_df['SA_Index'].str.contains(pat)]\n",
    "else:\n",
    "    # there are no non released sa to worry about.  This will return an empty df\n",
    "    logit.info(\"There are no unreleased parts or assemblies to worry about\")\n",
    "    rel_delta_df = delta_df\n",
    "\n",
    "\n",
    "# find blank source codes and drop the whole assembly\n",
    "empty_sc = set(rel_delta_df['SA_Index'][rel_delta_df['Source Code'].isna()].str.split('_').str[0])\n",
    "\n",
    "# remove any assemblies with empty source codes\n",
    "if len(empty_sc) > 0:\n",
    "    pat = '|'.join(r\"\\b^{}.*\\b\".format(x) for x in empty_sc)\n",
    "    rel_sc_delta_df = rel_delta_df[~rel_delta_df['SA_Index'].str.contains(pat)]\n",
    "\n",
    "else:\n",
    "    logit.info(\"There are no blank source codes to worry about\")\n",
    "    rel_sc_delta_df = rel_delta_df\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for Make without Buys\n",
    "\n",
    "IFS won't handle parents of Make where there are no child parts to buy\n",
    "\n",
    "if MAKE and there is no BUY below next row's part level less than or equal to current part level we have a MAKE without a BUY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_make_no_buy(df):\n",
    "    # if MAKE and there is no BUY below next row's part level less than or equal to current part level we have a MAKE without a BUY\n",
    "    # df['PROVIDE'] = np.where(df['Source Code'].isin(['AIH','MIH','MOB']),'Make','Buy')\n",
    "    make_no_buy = list(df[(df['Source Code'].isin(['AIH','MIH','MOB'])) & (df['Part Level'].shift(-1) <= df['Part Level'])].SA_Index)\n",
    "    make_no_buy = sorted(make_no_buy)\n",
    "    return make_no_buy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i think we can reset the index for delta_df without impacting \n",
    "make_no_buy = check_make_no_buy(delta_df)\n",
    "\n",
    "# remove any assemblies with make and no buy\n",
    "make_no_buy_df = pd.DataFrame()\n",
    "if len(make_no_buy) > 0:\n",
    "    pat = '|'.join(r\"\\b^{}.*\\b\".format(x.split('_')[0]) for x.split('_')[0] in make_no_buy)\n",
    "    rel_sc_delta_df = rel_sc_delta_df[~rel_sc_delta_df['SA_Index'].str.contains(pat)]\n",
    "\n",
    "    # get the make no buy part for writing out the warning messages\n",
    "    make_no_buy_df = delta_df[delta_df['SA_Index'] == x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass all the remaining parts left in the \n",
    "delta_parts_for_all_sa = rel_sc_delta_df['Part Number'].unique().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 Assemblies have changed\n",
      "9 changed assemblies that are not completely released so won't be migrated\n",
      "0 blank source codes\n",
      "0 Make parts without any Buy child parts\n"
     ]
    }
   ],
   "source": [
    "logit.info(\"{} Assemblies have changed\".format(len(sa_set)))\n",
    "logit.info(\"{} changed assemblies are not completely released so won't be processed\".format(len(unrel_sa_set)))\n",
    "logit.warning(\"{} blank source codes that will stop the whole assembly being released\".format(len(empty_sc)))\n",
    "logit.warning(\"{} Make parts without any Buy child parts that will stop the whole assembly being migrated\".format(len(make_no_buy)))\n",
    "if unrel_delta_df.shape[0] > 0:\n",
    "    for i, row in unrel_delta_df[['Function Group', 'Sub Group', 'Part Number', 'Parent Part', 'Release Status', 'SA_Index']].iterrows():\n",
    "        logit.warning(\"Part of Unreleased assembly and not processed: {} {} {} {} {}\".format(row['Function Group'], row['Sub Group'], row['Part Number'], row['Parent Part'], row['Release Status']))\n",
    "\n",
    "if make_no_buy_df.shape[0] > 0:\n",
    "    for i, row in make_no_buy_df[['Function Group', 'Sub Group', 'Part Number', 'Parent Part', 'Source Code', 'SA_Index']].iterrows():\n",
    "        logit.warning(\"Make Part with no buy child so assembly not processed: {} {} {} {} {}\".format(row['Function Group'], row['Sub Group'], row['Part Number'], row['Parent Part'], row['Source Code']))\n",
    "        \n",
    "print (\"{} Assemblies have changed\".format(len(sa_set)))\n",
    "print (\"{} changed assemblies that are not completely released so won't be migrated\".format(len(unrel_sa_set)))\n",
    "print (\"{} blank source codes\".format(len(empty_sc)))\n",
    "print (\"{} Make parts without any Buy child parts\".format(len(make_no_buy)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delta_all_parts = cleansed_df['Part Number'][cleansed_df['SA_Index'].str.contains('^{}'.format(sa_set))].to_list()\n",
    "# delta_parts_df = parts_df[parts_df['PART_NO'].isin(delta_parts_for_all_sa)]\n",
    "delta_structure_df = structure_df[structure_df['PART_NO'].isin(delta_parts_for_all_sa)]\n",
    "\n",
    "delta_all_parts = set(delta_parts_for_all_sa + delta_structure_df['SUB_PART_NO'].tolist())\n",
    "\n",
    "delta_parts_df = parts_df[parts_df['PART_NO'].isin(delta_all_parts)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_diff(data, color='pink'):\n",
    "    # Define html attribute\n",
    "    attr = 'background-color: {}'.format(color)\n",
    "    other = data.xs('Previous', axis='columns', level=-1)\n",
    "    # Where data != other set attribute\n",
    "    return pd.DataFrame(np.where((data.ne(other, level=0)), attr, ''),\n",
    "                        index=data.index, columns=data.columns)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_parts(df):\n",
    "    try:\n",
    "        df_all = pd.concat([df.loc[1,].set_index('PART_NO'), df.loc[2,].set_index('PART_NO')], axis='columns', keys=['Previous','Current'])\n",
    "        df_final = df_all.swaplevel(axis='columns')[delta_parts.columns[1:]].fillna('')\n",
    "    except:\n",
    "        df_all = pd.concat([pd.DataFrame('', columns=delta_parts.columns, index=delta_parts['PART_NO']), delta_parts.loc[2,].set_index('PART_NO')], axis='columns', keys=['Previous','Current'])\n",
    "        df_final = df_all.swaplevel(axis='columns')[delta_parts.columns[1:]].fillna('')\n",
    "\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_struct(df):\n",
    "    indx = ['PART_NO','SUB_PART_NO']\n",
    "    df_all = pd.concat([df.loc[1,].set_index(indx), df.loc[2,].set_index(indx)], axis='columns', keys=['Previous','Current'])\n",
    "    df_final = df_all.swaplevel(axis='columns')[df.drop(columns=indx).columns].fillna('')\n",
    "\n",
    "    return df_final"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a test file\n",
    "\n",
    "Provide a part number and will build test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "unterminated character set at position 8",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mark.chinnock\\gordonmurraygroup.com\\Engineering BoM - Documents\\bom\\BOM_to_IFS.ipynb Cell 97\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mark.chinnock/gordonmurraygroup.com/Engineering%20BoM%20-%20Documents/bom/BOM_to_IFS.ipynb#Y165sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# filter for test_part in PART_NO in structure file.  Store SUB_PART_NOs\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mark.chinnock/gordonmurraygroup.com/Engineering%20BoM%20-%20Documents/bom/BOM_to_IFS.ipynb#Y165sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mark.chinnock/gordonmurraygroup.com/Engineering%20BoM%20-%20Documents/bom/BOM_to_IFS.ipynb#Y165sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# test_parts = structure_df['SUB_PART_NO'][structure_df['PART_NO'] == 'T50-A8423'].to_list()\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mark.chinnock/gordonmurraygroup.com/Engineering%20BoM%20-%20Documents/bom/BOM_to_IFS.ipynb#Y165sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mark.chinnock/gordonmurraygroup.com/Engineering%20BoM%20-%20Documents/bom/BOM_to_IFS.ipynb#Y165sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# use the sa_index to find whole assembly\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mark.chinnock/gordonmurraygroup.com/Engineering%20BoM%20-%20Documents/bom/BOM_to_IFS.ipynb#Y165sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m SA_Index \u001b[39m=\u001b[39m cleansed_df[\u001b[39m'\u001b[39m\u001b[39mSA_Index\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mstr[\u001b[39m0\u001b[39m][cleansed_df[\u001b[39m'\u001b[39m\u001b[39mPart Number\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m test_part]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mark.chinnock/gordonmurraygroup.com/Engineering%20BoM%20-%20Documents/bom/BOM_to_IFS.ipynb#Y165sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m test_parts \u001b[39m=\u001b[39m cleansed_df[\u001b[39m'\u001b[39m\u001b[39mPart Number\u001b[39m\u001b[39m'\u001b[39m][cleansed_df[\u001b[39m'\u001b[39m\u001b[39mSA_Index\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mcontains(\u001b[39m'\u001b[39m\u001b[39m^\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(SA_Index))]\u001b[39m.\u001b[39mto_list()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mark.chinnock/gordonmurraygroup.com/Engineering%20BoM%20-%20Documents/bom/BOM_to_IFS.ipynb#Y165sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# add the test part to the list of SUB_PART_NOs\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mark.chinnock/gordonmurraygroup.com/Engineering%20BoM%20-%20Documents/bom/BOM_to_IFS.ipynb#Y165sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m test_parts\u001b[39m.\u001b[39mappend(test_part)\n",
      "File \u001b[1;32mc:\\Users\\mark.chinnock\\Anaconda3\\envs\\python311\\Lib\\site-packages\\pandas\\core\\strings\\accessor.py:136\u001b[0m, in \u001b[0;36mforbid_nonstring_types.<locals>._forbid_nonstring_types.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    131\u001b[0m     msg \u001b[39m=\u001b[39m (\n\u001b[0;32m    132\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCannot use .str.\u001b[39m\u001b[39m{\u001b[39;00mfunc_name\u001b[39m}\u001b[39;00m\u001b[39m with values of \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    133\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minferred dtype \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inferred_dtype\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    134\u001b[0m     )\n\u001b[0;32m    135\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 136\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mark.chinnock\\Anaconda3\\envs\\python311\\Lib\\site-packages\\pandas\\core\\strings\\accessor.py:1302\u001b[0m, in \u001b[0;36mStringMethods.contains\u001b[1;34m(self, pat, case, flags, na, regex)\u001b[0m\n\u001b[0;32m   1175\u001b[0m \u001b[39m@forbid_nonstring_types\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mbytes\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m   1176\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcontains\u001b[39m(\n\u001b[0;32m   1177\u001b[0m     \u001b[39mself\u001b[39m, pat, case: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m, flags: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, na\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, regex: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1178\u001b[0m ):\n\u001b[0;32m   1179\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1180\u001b[0m \u001b[39m    Test if pattern or regex is contained within a string of a Series or Index.\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1300\u001b[0m \u001b[39m    dtype: bool\u001b[39;00m\n\u001b[0;32m   1301\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1302\u001b[0m     \u001b[39mif\u001b[39;00m regex \u001b[39mand\u001b[39;00m re\u001b[39m.\u001b[39mcompile(pat)\u001b[39m.\u001b[39mgroups:\n\u001b[0;32m   1303\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1304\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mThis pattern is interpreted as a regular expression, and has \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1305\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mmatch groups. To actually get the groups, use str.extract.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1306\u001b[0m             \u001b[39mUserWarning\u001b[39;00m,\n\u001b[0;32m   1307\u001b[0m             stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m   1308\u001b[0m         )\n\u001b[0;32m   1310\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data\u001b[39m.\u001b[39marray\u001b[39m.\u001b[39m_str_contains(pat, case, flags, na, regex)\n",
      "File \u001b[1;32mc:\\Users\\mark.chinnock\\Anaconda3\\envs\\python311\\Lib\\re\\__init__.py:228\u001b[0m, in \u001b[0;36mcompile\u001b[1;34m(pattern, flags)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompile\u001b[39m(pattern, flags\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[0;32m    227\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mCompile a regular expression pattern, returning a Pattern object.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 228\u001b[0m     \u001b[39mreturn\u001b[39;00m _compile(pattern, flags)\n",
      "File \u001b[1;32mc:\\Users\\mark.chinnock\\Anaconda3\\envs\\python311\\Lib\\re\\__init__.py:307\u001b[0m, in \u001b[0;36m_compile\u001b[1;34m(pattern, flags)\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n\u001b[0;32m    302\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mThe re.TEMPLATE/re.T flag is deprecated \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    303\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mas it is an undocumented flag \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    304\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mwithout an obvious purpose. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    305\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mDon\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt use it.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    306\u001b[0m             \u001b[39mDeprecationWarning\u001b[39;00m)\n\u001b[1;32m--> 307\u001b[0m p \u001b[39m=\u001b[39m _compiler\u001b[39m.\u001b[39mcompile(pattern, flags)\n\u001b[0;32m    308\u001b[0m \u001b[39mif\u001b[39;00m flags \u001b[39m&\u001b[39m DEBUG:\n\u001b[0;32m    309\u001b[0m     \u001b[39mreturn\u001b[39;00m p\n",
      "File \u001b[1;32mc:\\Users\\mark.chinnock\\Anaconda3\\envs\\python311\\Lib\\re\\_compiler.py:743\u001b[0m, in \u001b[0;36mcompile\u001b[1;34m(p, flags)\u001b[0m\n\u001b[0;32m    741\u001b[0m \u001b[39mif\u001b[39;00m isstring(p):\n\u001b[0;32m    742\u001b[0m     pattern \u001b[39m=\u001b[39m p\n\u001b[1;32m--> 743\u001b[0m     p \u001b[39m=\u001b[39m _parser\u001b[39m.\u001b[39mparse(p, flags)\n\u001b[0;32m    744\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    745\u001b[0m     pattern \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mark.chinnock\\Anaconda3\\envs\\python311\\Lib\\re\\_parser.py:972\u001b[0m, in \u001b[0;36mparse\u001b[1;34m(str, flags, state)\u001b[0m\n\u001b[0;32m    969\u001b[0m state\u001b[39m.\u001b[39mflags \u001b[39m=\u001b[39m flags\n\u001b[0;32m    970\u001b[0m state\u001b[39m.\u001b[39mstr \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m\n\u001b[1;32m--> 972\u001b[0m p \u001b[39m=\u001b[39m _parse_sub(source, state, flags \u001b[39m&\u001b[39m SRE_FLAG_VERBOSE, \u001b[39m0\u001b[39m)\n\u001b[0;32m    973\u001b[0m p\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mflags \u001b[39m=\u001b[39m fix_flags(\u001b[39mstr\u001b[39m, p\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mflags)\n\u001b[0;32m    975\u001b[0m \u001b[39mif\u001b[39;00m source\u001b[39m.\u001b[39mnext \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\mark.chinnock\\Anaconda3\\envs\\python311\\Lib\\re\\_parser.py:453\u001b[0m, in \u001b[0;36m_parse_sub\u001b[1;34m(source, state, verbose, nested)\u001b[0m\n\u001b[0;32m    451\u001b[0m start \u001b[39m=\u001b[39m source\u001b[39m.\u001b[39mtell()\n\u001b[0;32m    452\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 453\u001b[0m     itemsappend(_parse(source, state, verbose, nested \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m,\n\u001b[0;32m    454\u001b[0m                        \u001b[39mnot\u001b[39;00m nested \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m items))\n\u001b[0;32m    455\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m sourcematch(\u001b[39m\"\u001b[39m\u001b[39m|\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    456\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mark.chinnock\\Anaconda3\\envs\\python311\\Lib\\re\\_parser.py:855\u001b[0m, in \u001b[0;36m_parse\u001b[1;34m(source, state, verbose, nested, first)\u001b[0m\n\u001b[0;32m    852\u001b[0m     group \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    853\u001b[0m sub_verbose \u001b[39m=\u001b[39m ((verbose \u001b[39mor\u001b[39;00m (add_flags \u001b[39m&\u001b[39m SRE_FLAG_VERBOSE)) \u001b[39mand\u001b[39;00m\n\u001b[0;32m    854\u001b[0m                \u001b[39mnot\u001b[39;00m (del_flags \u001b[39m&\u001b[39m SRE_FLAG_VERBOSE))\n\u001b[1;32m--> 855\u001b[0m p \u001b[39m=\u001b[39m _parse_sub(source, state, sub_verbose, nested \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m    856\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m source\u001b[39m.\u001b[39mmatch(\u001b[39m\"\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    857\u001b[0m     \u001b[39mraise\u001b[39;00m source\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mmissing ), unterminated subpattern\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    858\u001b[0m                        source\u001b[39m.\u001b[39mtell() \u001b[39m-\u001b[39m start)\n",
      "File \u001b[1;32mc:\\Users\\mark.chinnock\\Anaconda3\\envs\\python311\\Lib\\re\\_parser.py:453\u001b[0m, in \u001b[0;36m_parse_sub\u001b[1;34m(source, state, verbose, nested)\u001b[0m\n\u001b[0;32m    451\u001b[0m start \u001b[39m=\u001b[39m source\u001b[39m.\u001b[39mtell()\n\u001b[0;32m    452\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 453\u001b[0m     itemsappend(_parse(source, state, verbose, nested \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m,\n\u001b[0;32m    454\u001b[0m                        \u001b[39mnot\u001b[39;00m nested \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m items))\n\u001b[0;32m    455\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m sourcematch(\u001b[39m\"\u001b[39m\u001b[39m|\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    456\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mark.chinnock\\Anaconda3\\envs\\python311\\Lib\\re\\_parser.py:561\u001b[0m, in \u001b[0;36m_parse\u001b[1;34m(source, state, verbose, nested, first)\u001b[0m\n\u001b[0;32m    559\u001b[0m this \u001b[39m=\u001b[39m sourceget()\n\u001b[0;32m    560\u001b[0m \u001b[39mif\u001b[39;00m this \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 561\u001b[0m     \u001b[39mraise\u001b[39;00m source\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39munterminated character set\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    562\u001b[0m                        source\u001b[39m.\u001b[39mtell() \u001b[39m-\u001b[39m here)\n\u001b[0;32m    563\u001b[0m \u001b[39mif\u001b[39;00m this \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mset\u001b[39m:\n\u001b[0;32m    564\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;31merror\u001b[0m: unterminated character set at position 8"
     ]
    }
   ],
   "source": [
    "# test_part = 'T50-A8115'\n",
    "# # filter for test_part in PART_NO in structure file.  Store SUB_PART_NOs\n",
    "\n",
    "# # test_parts = structure_df['SUB_PART_NO'][structure_df['PART_NO'] == 'T50-A8423'].to_list()\n",
    "\n",
    "# # use the sa_index to find whole assembly\n",
    "# SA_Index = cleansed_df['SA_Index'].str.split('_').str[0][cleansed_df['Part Number'] == test_part]\n",
    "# test_parts = cleansed_df['Part Number'][cleansed_df['SA_Index'].str.contains('^{}'.format(SA_Index))].to_list()\n",
    "# # add the test part to the list of SUB_PART_NOs\n",
    "# test_parts.append(test_part)\n",
    "# # go back to structure file and look for list of parts in PART_NO\n",
    "# test_structure_df = structure_df[structure_df['PART_NO'].isin(test_parts)]\n",
    "\n",
    "# all_test_parts = set(test_parts + test_structure_df['SUB_PART_NO'].tolist())\n",
    "\n",
    "# test_parts_df = parts_df[parts_df['PART_NO'].isin(all_test_parts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Function Group</th>\n",
       "      <th>Sub Group</th>\n",
       "      <th>Part Level</th>\n",
       "      <th>Part Number</th>\n",
       "      <th>Issue Level</th>\n",
       "      <th>Part Description</th>\n",
       "      <th>Parent Part</th>\n",
       "      <th>Source Code</th>\n",
       "      <th>Weight (KG)</th>\n",
       "      <th>Release Status</th>\n",
       "      <th>Engineer</th>\n",
       "      <th>orig_sort</th>\n",
       "      <th>Service Identifier</th>\n",
       "      <th>Engineer lookup</th>\n",
       "      <th>Part Type</th>\n",
       "      <th>SA_Index</th>\n",
       "      <th>BOF_Parent</th>\n",
       "      <th>Part - Qty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>T50-CAR</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CAR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SYS</td>\n",
       "      <td>9947.174601</td>\n",
       "      <td>REL</td>\n",
       "      <td>Engineer Name</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan_T50-CAR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>BODY SYSTEMS</td>\n",
       "      <td>2</td>\n",
       "      <td>T50-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>BODY SYSTEMS</td>\n",
       "      <td>T50-CAR</td>\n",
       "      <td>SYS</td>\n",
       "      <td>1138.074684</td>\n",
       "      <td>REL</td>\n",
       "      <td>Engineer Name</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan_T50-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>CHASSIS SYSTEMS</td>\n",
       "      <td>2</td>\n",
       "      <td>T50-02</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CHASSIS SYSTEMS</td>\n",
       "      <td>T50-CAR</td>\n",
       "      <td>SYS</td>\n",
       "      <td>4731.439927</td>\n",
       "      <td>REL</td>\n",
       "      <td>Engineer Name</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan_T50-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>POWERTRAIN SYSTEMS</td>\n",
       "      <td>2</td>\n",
       "      <td>T50-03</td>\n",
       "      <td>1.0</td>\n",
       "      <td>POWERTRAIN SYSTEMS</td>\n",
       "      <td>T50-CAR</td>\n",
       "      <td>SYS</td>\n",
       "      <td>839.815392</td>\n",
       "      <td>REL</td>\n",
       "      <td>Engineer Name</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan_T50-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>ELECTRICAL SYSTEMS</td>\n",
       "      <td>2</td>\n",
       "      <td>T50-04</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ELECTRICAL SYSTEMS</td>\n",
       "      <td>T50-CAR</td>\n",
       "      <td>SYS</td>\n",
       "      <td>119.750237</td>\n",
       "      <td>REL</td>\n",
       "      <td>Engineer Name</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan_T50-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11899</th>\n",
       "      <td>Powertrain</td>\n",
       "      <td>L01-Cooling Systems</td>\n",
       "      <td>6</td>\n",
       "      <td>TFF-AA040</td>\n",
       "      <td>1.0</td>\n",
       "      <td>K-NUT-FLANGE-M8x1.25</td>\n",
       "      <td>T50-L1367</td>\n",
       "      <td>FAS</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>REL</td>\n",
       "      <td>AH</td>\n",
       "      <td>40075.0</td>\n",
       "      <td>Y</td>\n",
       "      <td>AD</td>\n",
       "      <td>Purchased (Raw)</td>\n",
       "      <td>40073.0_TFF-AA040</td>\n",
       "      <td>T50-L1369</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11900</th>\n",
       "      <td>Powertrain</td>\n",
       "      <td>L01-Cooling Systems</td>\n",
       "      <td>6</td>\n",
       "      <td>TPP-LZ065</td>\n",
       "      <td>1.0</td>\n",
       "      <td>SEAL-BONDED-DASH 6</td>\n",
       "      <td>T50-L1367</td>\n",
       "      <td>BOP</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>REL</td>\n",
       "      <td>AC</td>\n",
       "      <td>40076.0</td>\n",
       "      <td>Y</td>\n",
       "      <td>ADC</td>\n",
       "      <td>Purchased (Raw)</td>\n",
       "      <td>40073.0_TPP-LZ065</td>\n",
       "      <td>T50-L1369</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11901</th>\n",
       "      <td>Powertrain</td>\n",
       "      <td>L01-Cooling Systems</td>\n",
       "      <td>5</td>\n",
       "      <td>T50-L0381</td>\n",
       "      <td>3.0</td>\n",
       "      <td>AM-PLATE-CLAMP-CONDENSER PIPES</td>\n",
       "      <td>T50-L01-03</td>\n",
       "      <td>AIH</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>REL</td>\n",
       "      <td>BH</td>\n",
       "      <td>40077.0</td>\n",
       "      <td>N</td>\n",
       "      <td>NO_ENGINEER</td>\n",
       "      <td>Manufactured</td>\n",
       "      <td>40077.0_T50-L0381</td>\n",
       "      <td>T50-L1369</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11902</th>\n",
       "      <td>Powertrain</td>\n",
       "      <td>L01-Cooling Systems</td>\n",
       "      <td>6</td>\n",
       "      <td>T50-L0373</td>\n",
       "      <td>1.0</td>\n",
       "      <td>PLATE-CLAMP-CONDENSER PIPES</td>\n",
       "      <td>T50-L0381</td>\n",
       "      <td>BOF</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>REL</td>\n",
       "      <td>AC</td>\n",
       "      <td>40078.0</td>\n",
       "      <td>Y</td>\n",
       "      <td>NO_ENGINEER</td>\n",
       "      <td>Purchased (Raw)</td>\n",
       "      <td>40077.0_T50-L0373</td>\n",
       "      <td>T50-L0373</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11903</th>\n",
       "      <td>Powertrain</td>\n",
       "      <td>L01-Cooling Systems</td>\n",
       "      <td>6</td>\n",
       "      <td>TFF-SA888</td>\n",
       "      <td>1.0</td>\n",
       "      <td>BOLT-CAP HD-TORX-M6X20X1.0(Ti)</td>\n",
       "      <td>T50-L0381</td>\n",
       "      <td>FAS</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>REL</td>\n",
       "      <td>NF</td>\n",
       "      <td>40079.0</td>\n",
       "      <td>Y</td>\n",
       "      <td>PB</td>\n",
       "      <td>Purchased (Raw)</td>\n",
       "      <td>40077.0_TFF-SA888</td>\n",
       "      <td>T50-L0373</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11598 rows  18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Function Group            Sub Group  Part Level Part Number  \\\n",
       "0               None                 None           1     T50-CAR   \n",
       "1               None         BODY SYSTEMS           2      T50-01   \n",
       "2               None      CHASSIS SYSTEMS           2      T50-02   \n",
       "3               None   POWERTRAIN SYSTEMS           2      T50-03   \n",
       "4               None   ELECTRICAL SYSTEMS           2      T50-04   \n",
       "...              ...                  ...         ...         ...   \n",
       "11899     Powertrain  L01-Cooling Systems           6   TFF-AA040   \n",
       "11900     Powertrain  L01-Cooling Systems           6   TPP-LZ065   \n",
       "11901     Powertrain  L01-Cooling Systems           5   T50-L0381   \n",
       "11902     Powertrain  L01-Cooling Systems           6   T50-L0373   \n",
       "11903     Powertrain  L01-Cooling Systems           6   TFF-SA888   \n",
       "\n",
       "       Issue Level                Part Description Parent Part Source Code  \\\n",
       "0              1.0                             CAR         NaN         SYS   \n",
       "1              1.0                    BODY SYSTEMS     T50-CAR         SYS   \n",
       "2              1.0                 CHASSIS SYSTEMS     T50-CAR         SYS   \n",
       "3              1.0              POWERTRAIN SYSTEMS     T50-CAR         SYS   \n",
       "4              1.0              ELECTRICAL SYSTEMS     T50-CAR         SYS   \n",
       "...            ...                             ...         ...         ...   \n",
       "11899          1.0            K-NUT-FLANGE-M8x1.25   T50-L1367         FAS   \n",
       "11900          1.0              SEAL-BONDED-DASH 6   T50-L1367         BOP   \n",
       "11901          3.0  AM-PLATE-CLAMP-CONDENSER PIPES  T50-L01-03         AIH   \n",
       "11902          1.0     PLATE-CLAMP-CONDENSER PIPES   T50-L0381         BOF   \n",
       "11903          1.0  BOLT-CAP HD-TORX-M6X20X1.0(Ti)   T50-L0381         FAS   \n",
       "\n",
       "       Weight (KG) Release Status       Engineer  orig_sort  \\\n",
       "0      9947.174601            REL  Engineer Name        NaN   \n",
       "1      1138.074684            REL  Engineer Name        NaN   \n",
       "2      4731.439927            REL  Engineer Name        NaN   \n",
       "3       839.815392            REL  Engineer Name        NaN   \n",
       "4       119.750237            REL  Engineer Name        NaN   \n",
       "...            ...            ...            ...        ...   \n",
       "11899     0.003000            REL             AH    40075.0   \n",
       "11900     0.002000            REL             AC    40076.0   \n",
       "11901     0.000000            REL             BH    40077.0   \n",
       "11902     0.012000            REL             AC    40078.0   \n",
       "11903     0.003000            REL             NF    40079.0   \n",
       "\n",
       "      Service Identifier Engineer lookup        Part Type           SA_Index  \\\n",
       "0                    NaN             NaN              NaN        nan_T50-CAR   \n",
       "1                    NaN             NaN              NaN         nan_T50-01   \n",
       "2                    NaN             NaN              NaN         nan_T50-02   \n",
       "3                    NaN             NaN              NaN         nan_T50-03   \n",
       "4                    NaN             NaN              NaN         nan_T50-04   \n",
       "...                  ...             ...              ...                ...   \n",
       "11899                  Y              AD  Purchased (Raw)  40073.0_TFF-AA040   \n",
       "11900                  Y             ADC  Purchased (Raw)  40073.0_TPP-LZ065   \n",
       "11901                  N     NO_ENGINEER     Manufactured  40077.0_T50-L0381   \n",
       "11902                  Y     NO_ENGINEER  Purchased (Raw)  40077.0_T50-L0373   \n",
       "11903                  Y              PB  Purchased (Raw)  40077.0_TFF-SA888   \n",
       "\n",
       "      BOF_Parent  Part - Qty  \n",
       "0            NaN         NaN  \n",
       "1            NaN         1.0  \n",
       "2            NaN         1.0  \n",
       "3            NaN         1.0  \n",
       "4            NaN         1.0  \n",
       "...          ...         ...  \n",
       "11899  T50-L1369         1.0  \n",
       "11900  T50-L1369         2.0  \n",
       "11901  T50-L1369         1.0  \n",
       "11902  T50-L0373         1.0  \n",
       "11903  T50-L0373         1.0  \n",
       "\n",
       "[11598 rows x 18 columns]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleansed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Writing out changes since 20230808-1508 file ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mark.chinnock\\AppData\\Local\\Temp\\ipykernel_6700\\3233650185.py:140: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  df_final_struct.loc[1,]\n"
     ]
    }
   ],
   "source": [
    "# Validation checks before writing out.  Don't write files without an error_count of zero\n",
    "\n",
    "TEST=False\n",
    "DELTA=True\n",
    "\n",
    "if TEST:\n",
    "    print (\"*** TEST MODE ***\")\n",
    "    out_structure_df = test_structure_df\n",
    "    out_parts_df = test_parts_df\n",
    "else:\n",
    "    out_structure_df = structure_df\n",
    "    out_parts_df = parts_df\n",
    "     \n",
    "\n",
    "error_count = 0\n",
    "\n",
    "# check for zero quantities\n",
    "zero_quantities = out_structure_df[out_structure_df['QTY'] == 0]\n",
    "if zero_quantities.shape[0] > 0:\n",
    "    zero_quantities.to_excel(os.path.join(base, '{}_Zero_Quantity.xlsx'.format(project)))\n",
    "    logit.error(\"Zero Quantities found - needs resolving first!\")\n",
    "    print (\"Zero Quantities found - needs resolving first!\")\n",
    "    error_count += 1\n",
    "\n",
    "# check for decimal quantities\n",
    "decimal_quantities = out_structure_df[(out_structure_df['QTY'] > 0) & (out_structure_df['QTY'] < 1)]\n",
    "if decimal_quantities.shape[0] > 0:\n",
    "    decimal_quantities.to_excel(os.path.join(base, '{}_Decimal_Quantity.xlsx'.format(project)))\n",
    "    logit.error(\"Decimal Quantities found - needs resolving first!\")\n",
    "    print (\"Decimal Quantities found - needs resolving first!\")\n",
    "    error_count += 1\n",
    "\n",
    "# check all parts present\n",
    "orphaned_parts = []\n",
    "orphaned_parts = pd.merge(out_parts_df, out_structure_df, left_on='PART_NO', right_on='SUB_PART_NO', how='left', indicator=True)\n",
    "orphaned_parts = orphaned_parts[['PART_NO_x','SUB_PART_NO']][orphaned_parts['_merge'] == 'left_only']\n",
    "if orphaned_parts.shape[0] > 1:\n",
    "    logit.error(\"Expecting just the top CAR part to not have any parent\")\n",
    "    logit.info(\"orphaned parts {}\".format(orphaned_parts))\n",
    "    print (\"Expecting just the top CAR part to not have any parent\")\n",
    "    print (orphaned_parts)\n",
    "    print (\"\")\n",
    "    error_count =+ 1\n",
    "\n",
    "# check all master parts have child parts\n",
    "no_child_part = []\n",
    "no_child_part = pd.merge(out_structure_df, out_parts_df, left_on='SUB_PART_NO', right_on='PART_NO', how='left', indicator=True)\n",
    "no_child_part = no_child_part[['PART_NO_x','SUB_PART_NO']][no_child_part['_merge'] == 'left_only']\n",
    "if no_child_part.shape[0] > 0:\n",
    "    logit.error (\"Not expecting any parts without sub parts\")\n",
    "    logit.error (no_child_part)\n",
    "    print (\"Not expecting any parts without sub parts\")\n",
    "    print (no_child_part)\n",
    "    error_count =+ 1\n",
    "\n",
    "# st\n",
    "sub_part_rev_check = []\n",
    "sub_part_rev_check = pd.merge(out_structure_df, out_parts_df, left_on=['SUB_PART_NO','SUB_PART_REV'], right_on=['PART_NO','ENG_PART_REV'], indicator=True, how='left')\n",
    "sub_part_rev_check = sub_part_rev_check[sub_part_rev_check['_merge'] == 'left_only']\n",
    "if sub_part_rev_check.shape[0] > 0:\n",
    "    logit.error (\"SUB_PART_REV and ENG_PART_REV do not match\")\n",
    "    logit.error (sub_part_rev_check)\n",
    "    print (\"SUB_PART_REV and ENG_PART_REV do not match\")\n",
    "    print (sub_part_rev_check)\n",
    "    error_count =+ 1        \n",
    "\n",
    "# check sub part rev matches in parts and structure files\n",
    "part_rev_check = out_structure_df[out_structure_df.PART_REV.isna()]\n",
    "if part_rev_check.shape[0] > 0:\n",
    "    logit.error (\"Can't have a blank PART_REV for these parts in structure file\")\n",
    "    logit.error (part_rev_check)\n",
    "    print (\"Can't have a blank PART_REV for these parts in structure file\")\n",
    "    print (part_rev_check)\n",
    "    error_count =+ 1\n",
    "\n",
    "# find master parts in parts_df and check PART_REV and SUB_PART_REV match\n",
    "master_part = out_structure_df[['PART_REV','PART_NO']]\n",
    "missing_master = pd.merge(master_part, out_structure_df, left_on=['PART_REV','PART_NO'], right_on=['SUB_PART_REV','SUB_PART_NO'], how='left', indicator=True)\n",
    "missing_master = missing_master[missing_master['_merge'] == 'left_only']\n",
    "missing_master = missing_master[~missing_master['PART_NO_x'].str.contains('-CAR')]\n",
    "if TEST:\n",
    "    # for a test file we won't need the structure for the actual assembly we've specified\n",
    "    missing_master = missing_master[~missing_master['PART_NO_x'].str.contains(test_part)]\n",
    "\n",
    "if missing_master.shape[0] > 0:\n",
    "    logit.error (\"missing_master: Must find PART_REV / PART_NO combo in SUB_PART_REV / SUB_PART_NO\")\n",
    "    logit.error (missing_master)\n",
    "    print (\"missing_master: Must find PART_REV / PART_NO combo in SUB_PART_REV / SUB_PART_NO\")\n",
    "    print (missing_master)\n",
    "    error_count =+ 1   \n",
    "\n",
    "\n",
    "if error_count == 0: \n",
    "\n",
    "    outfile_part = 'Part_{}_{}_{}_{}'.format(project, env, incrementer, timestr)\n",
    "    outfile_structure = 'Structure_{}_{}_{}_{}'.format(project, env, incrementer, timestr)\n",
    "\n",
    "    if TEST:\n",
    "        outfile_part = outfile_part + '_' + test_part\n",
    "        outfile_structure = outfile_structure + '_' + test_part\n",
    "\n",
    "    def write_to_excel(df, outfile):\n",
    "        with pd.ExcelWriter(os.path.join(outdir, outfile), engine=\"openpyxl\") as writer:\n",
    "                df.to_excel(writer, sheet_name = 'Sheet1', index=False)\n",
    "                ws = writer.sheets['Sheet1']\n",
    "                wb = writer.book\n",
    "                excel_formatting.adjust_col_width_from_col(ws)\n",
    "    \n",
    "    def write_to_csv(df, outfile):\n",
    "        df.to_csv(os.path.join(outdir, outfile),sep='\\t', index=False, encoding='utf-8')\n",
    "        # parts_df.to_csv(os.path.join(outdir, outfile),sep='\\t', index=False, encoding='utf-8')\n",
    "\n",
    "    if DELTA:\n",
    "        logit.info(\"*** Writing out changes since {} file ***\".format(prev_timestamps))\n",
    "        print (\"*** Writing out changes since {} file ***\".format(prev_timestamps))\n",
    "        write_to_excel(delta_parts_df, 'DELTA_{}.xlsx'.format(outfile_part))\n",
    "        write_to_csv(delta_parts_df, 'DELTA_{}.txt'.format(outfile_part))\n",
    "        write_to_excel(delta_structure_df, 'DELTA_{}.xlsx'.format(outfile_structure))\n",
    "        write_to_csv(delta_structure_df, 'DELTA_{}.txt'.format(outfile_structure))\n",
    "\n",
    "        # write out the highlighted changes\n",
    "        compare_file = 'COMPARE_{}_vs_{}.xlsx'.format(prev_timestamps, timestr)\n",
    "        compare_out = os.path.join(outdir, compare_file)\n",
    "        df_final_parts = compare_parts(delta_parts)\n",
    "        try:\n",
    "            delta_struct.loc[1,]\n",
    "            try:\n",
    "                delta_struct.loc[2,]\n",
    "                df_final_struct = compare_struct(delta_struct)\n",
    "            except (KeyError):\n",
    "                print (\"No current parts - assumed all changes are parts being removed\")\n",
    "                # nothing to compare so just provide an empty delta_struct\n",
    "                df_final_struct = delta_struct\n",
    "        except (KeyError) as e:\n",
    "            print (\"No previous parts - assumed all changes are parts being added\")\n",
    "            df_final_struct = delta_struct\n",
    "        # df_final_part.style.apply(highlight_diff, axis=None).to_excel(compare_out, engine='openpyxl',)\n",
    "        with pd.ExcelWriter(compare_out) as writer:\n",
    "            try:\n",
    "                df_final_struct.loc[1,]\n",
    "                try:\n",
    "                    df_final_struct.loc[2,]\n",
    "                    # highlight the differences between index 1 and 2\n",
    "                    df_final_struct.style.apply(highlight_diff, axis=None).to_excel(writer, sheet_name='Structure')\n",
    "                except (KeyError) as e:\n",
    "                    # nothing in 2 to compare.  Write out the structures involved with the parts\n",
    "                    delta_structure_df.to_excel(writer, sheet_name='Structure')\n",
    "            except (KeyError) as e:\n",
    "                # nothing in 1 to compare.  Write out the structures involved with the parts\n",
    "                delta_structure_df.to_excel(writer, sheet_name='Structure')\n",
    "\n",
    "            df_final_parts.style.apply(highlight_diff, axis=None).to_excel(writer, sheet_name='Parts')\n",
    "\n",
    "        # write out the delta df file for Lorena to file a bom-like file\n",
    "        delta_bom_file = 'Changed_Assemblies_BOM_{}_{}_vs_{}.xlsx'.format(project, prev_timestamps, timestr)\n",
    "        delta_bom_out = os.path.join(outdir, delta_bom_file)\n",
    "        write_to_excel(delta_df, delta_bom_out)\n",
    "\n",
    "    write_to_excel(out_parts_df, outfile_part + '.xlsx')\n",
    "    write_to_csv(out_parts_df, outfile_part + '.txt')\n",
    "    write_to_excel(out_structure_df, outfile_structure + '.xlsx')\n",
    "    write_to_csv(out_structure_df, outfile_structure + '.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit.info('Completed')\n",
    "\n",
    "for handler in logit.handlers:\n",
    "    if isinstance(handler, logging.FileHandler):\n",
    "        handler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for handler in logit.handlers:\n",
    "    if isinstance(handler, logging.FileHandler):\n",
    "        handler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "64e4acd0b8bcdde64ca4122ca150d77580571c820a6f3cf10fee72812efda0cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
